model_id: eeg_things_eeg_2_vit_b_32
modality: eeg
dataset: things_eeg_2
features: vision transformer (ViT-B/32)
repeats: multi
subject_specific: true

# General description of the model
description: |
  This model generates in silico EEG responses to visual stimuli using a vision transformer model.
  It was trained on the THINGS-EEG-2 dataset, which contains EEG recordings from subjects viewing
  images of everyday objects. The model extracts visual features using a pre-trained ViT-B/32
  transformer, applies dimensionality reduction, and then predicts EEG responses across all channels
  and time points.

  The model takes as input a batch of RGB images in the shape [batch_size, 3, height, width], with pixel values ranging from 0 to 255 and square dimensions (e.g., 224Ã—224).

# Input stimulus information
input:
  type: "numpy.ndarray"
  shape: [batch_size, 3, height, width]
  description: "The input should be a batch of RGB images."
  constraints:
    - "Image values should be integers in range [0, 255]"
    - "Image dimensions (height, width) should be equal (square)"
    - "Minimum recommended image size: 224x224 pixels"

# Output information
output:
  type: "numpy.ndarray"
  shape: [batch_size, n_repetitions, n_channels, n_timepoints]
  description: "The output is a 4D array containing predicted EEG responses."
  dimensions:
    - name: "batch_size"
      description: "Number of stimuli in the batch"
    - name: "n_repetitions"
      description: "Number of simulated repetitions of the same stimulus (typically 4)"
    - name: "n_channels"
      description: "Number of EEG channels (typically 64)"
    - name: "n_timepoints"
      description: "Number of time points in the EEG epoch (typically 140)"

# Model parameters and their usage
parameters:
  subject:
    type: int
    required: true
    valid_values: [1, 2, 3, 4]
    example: 1
    description: "Subject ID from the THINGS-EEG-2 dataset (1-4)"
    function: "get_encoding_model"
  
  nest_dir:
    type: str
    required: false
    example: "./"
    description: "Root directory of the NEST repository (optional if default paths are set)"
    function: "get_encoding_model"

  stimulus:
    type: numpy.ndarray
    required: true
    shape: [batch_size, 3, height, width]
    description: "A batch of RGB images to be encoded. Images should be in integer format with values in the range [0, 255], and square dimensions (e.g. 224x224)."
    example: "An array of shape [100, 3, 224, 224] representing 100 RGB images."
    function: "encode"

  device:
    type: str
    required: false
    valid_values: ["cpu", "cuda", "auto"]
    default: "auto"
    example: "auto"
    description: "Device to run the model on. 'auto' will use CUDA if available, otherwise CPU."
    function: "encode"
  
  show_progress:
    type: bool
    required: false
    default: True
    example: True
    description: "Whether to show a progress bar during encoding (for large batches)"
    function: "encode"

# Performance metrics and references
performance:
  
  accuracy_plots: 
    - "neural_encoding_simulation_toolkit/encoding_models/modality-eeg/train_dataset-things_eeg_2/model-vit_b_32/encoding_models_accuracy"
  
references:
    - x