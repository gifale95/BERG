model_id: fmri-nsd_fsaverage-vit_b_32
modality: fMRI
training_dataset: Natural Scenes Dataset (NSD) (fsaverage surface space)
species: Human
stimuli: Images
model_type: Vision transformer (ViT-B/32)
creator: Alessandro Gifford

# General description of the model
description: |
  These encoding models consist in a linear mapping (through linear regression) of vision transformer
  (Dosovitskiy et al., 2020) image features onto fMRI responses. Prior to mapping onto fMRI responses, the
  image features have been downsampled to 250 principal components using principal component analysis.
  
  The encoding models were trained on the Natural Scenes Dataset (NSD) (Allen et al., 2022), 7T fMRI responses of 8
  subjects to 73k natural scenes coming from the COCO dataset (Lin et al., 2014). One encoding model was trained for
  each NSD subject, and for each fMRI vertex.
  
  **Preprocessing.** The encoding models are trained on NSD's data prepared in FreeSurfer's fsaverage space, from the
  “betas_fithrf_GLMdenoise_RR” preprocessing version. Note that the NSD data were *z*-scored at each scan session, and as
  a consequence the in silico fMRI responses generated by the encoding models also live in *z*-scored space.
  
  **Model training partition.** fMRI responses for up to 9,000 non-shared images (i.e., the images uniquely seen by each
  subject during the NSD experiment).
  
  **Model validation partition.** fMRI responses for up to 485/1,000 shared images (i.e., the 485 shared images that not
  all subjects saw for up to three times during the NSD experiment).
  
  **Model testing partition.** fMRI responses for 515/1,000 shared images (i.e., the 515 images that each subject saw for
  exactly three times during the NSD experiment).

# Input stimulus information
input:
  type: "numpy.ndarray"
  shape: [batch_size, 3, height, width]
  description: "The input should be a batch of RGB images."
  constraints:
    - "Image values should be integers in range [0, 255]."
    - "Image dimensions (height, width) should be equal (square)."
    - "Minimum recommended image size: 224×224 pixels."

# Output information
output:
  type: "tuple of numpy.ndarray"
  shape: ([batch_size, lh_vertices], [batch_size, rh_vertices])
  description: |
    The output is a tuple containing the left hemisphere (LH) and right hemisphere (RH) in silico fMRI
    responses for the batch images.
  dimensions:
    - name: "batch_size"
      description: "Number of stimuli in the batch."
    - name: "lh_vertices"
      description: "Number of selected LH vertices for which the in silico fMRI responses are generated."
    - name: "rh_vertices"
      description: "Number of selected RH vertices for which the in silico fMRI responses are generated."

# Model parameters and their usage
parameters:
  subject:
    type: int
    required: true
    valid_values: [1, 2, 3, 4, 5, 6, 7, 8]
    example: 1
    description: "Subject ID from the NSD dataset (1-8)."
    function: "get_encoding_model"

  selection:
    type: dict
    description: Specifies which outputs to include in the model responses. If not provided, fMRI responses
      are generate for all LH and RH fMRI vertices.
    required: false
    function: get_encoding_model
    properties:
      roi:
        type: str
        description: |
          The region-of-interest (ROI) for which the in silico fMRI responses (of both
          hemispherese) are generated.
        valid_values: [
          "V1d", "V1v", "V2d", "V2v", "V3d", "V3v", "hV4", "OFA", "FFA-1", "FFA-2",
          "mTL-faces", "aTL-faces", "OVWFA", "VWFA-1", "VWFA-2", "mfs-words", "mTL-words",
          "OPA","PPA","RSC","EBA","FBA-1","FBA-2","mTL-bodies","early", "midventral",
          "midlateral", "midparietal", "parietal", "lateral", "ventral"]
        example_roi: "V1v"
      lh_vertices:
        type: numpy.ndarray
        description: |
          Binary one-hot encoded vector with ones indicating the left hemisphere (LH)
          vertices for which the in silico fMRI responses are generated. This vector must
          have exactly the same length as the number of LH fsaverage vertices (163,842).
          The vertices from the one-hot encoded vector are only selected if the "roi" key
          is not provided, or has value None.
        example_vertices: [0, 0, ..., 1, 1, 0]
      rh_vertices:
        type: numpy.ndarray
        description: |
          Binary one-hot encoded vector with ones indicating the right hemisphere (RH)
          vertices for which the in silico fMRI responses are generated. This vector must
          have exactly the same length as the number of RH fsaverage vertices (163,842).
          The vertices from the one-hot encoded vector are only selected if the "roi" key
          is not provided, or has value None.
        example_vertices: [0, 0, ..., 1, 1, 0]

  stimulus:
    type: numpy.ndarray
    required: true
    shape: [batch_size, 3, height, width]
    description: "A batch of RGB images to be encoded. Images should be in integer format with values in the range [0, 255], and square dimensions (e.g. 224x224)."
    example: "An array of shape [100, 3, 224, 224] representing 100 RGB images."
    function: "encode"

  device:
    type: str
    required: false
    valid_values: ["cpu", "cuda", "auto"]
    default: "auto"
    example: "auto"
    description: "Device to run the model on. 'auto' will use CUDA if available, otherwise CPU."
    function: "encode"

# Performance metrics and references
performance:

  accuracy_plots: 
    - "brain-encoding-response-generator/encoding_models/modality-fmri/train_dataset-nsd/model-vit_b_32/encoding_models_accuracy"
  
references:
    - Model building code: 'https://github.com/gifale95/BERG/tree/main/berg_creation_code'
    - NSD paper (Allen et al., 2022): https://doi.org/10.1038/s41593-021-00962-x
    - ViT-B/32 (Dosovitskiy et al., 2020): https://arxiv.org/abs/2010.11929
    - COCO dataset (Lin et al., 2014): https://cocodataset.org/#home
