model_id: eeg-things_eeg_2-vit_b_32
modality: EEG
training_dataset:  THINGS EEG2
species: Human
stimuli: Images
model_type: Vision transformer (ViT-B/32)
creator: Alessandro Gifford

# General description of the model
description: |
  These encoding models consist in a linear mapping (through linear regression) of vision transformer
  (Dosovitskiy et al., 2020) image features onto EEG responses. Prior to mapping onto EEG responses, the
  image features have been downsampled to 250 principal components using principal component analysis.

  The encoding models were trained on THINGS EEG2 (Gifford et al., 2022), 63-channel EEG responses of 10 subjects to
  over 16,740 images from the THINGS initiative (Hebart et al., 2019).
  
  **Preprocessing**. During preprocessing the 63-channel raw EEG data was filtered between 0.03 Hz and 100 Hz; epoched
  from -100 ms to +600 ms with respect to stimulus onset; transformed using current source density transform;
  downsampled to 200 Hz resulting in 140 times points per epoch (one every 5 ms); baseline corrected at each channel
  using the mean of the pre-stimulus interval.

  **Model training partition.** EEG responses for 16,540 unique images, each repeated 4 times (i.e., the official
  training partition of the THINGS EEG2 dataset).

  **Model testing partition.** EEG responses for 200 unique images, each repeated 80 times (i.e., the official testing
  partition of the THINGS EEG2 dataset).

  Independent encoding models were trained for each of the 4 training data repeats, and as a result the trained encoding models 
  generate 4 instances (i.e., repeats) of in silico EEG responses.  Indepedent encoding models were trained for each subject,
  channel, and time point.

# Input stimulus information
input:
  type: "numpy.ndarray"
  shape: [batch_size, 3, height, width]
  description: "The input should be a batch of RGB images."
  constraints:
    - "Image values should be integers in range [0, 255]."
    - "Image dimensions (height, width) should be equal (square)."
    - "Minimum recommended image size: 224×224 pixels."

# Output information
output:
  type: "numpy.ndarray"
  shape: [batch_size, n_repetitions, n_channels, n_timepoints]
  description: "The output is a 4D array containing in silico EEG responses."
  dimensions:
    - name: "batch_size"
      description: "Number of stimuli in the batch."
    - name: "n_repetitions"
      description: "Number of simulated repetitions of the same stimulus (always 4)."
    - name: "n_channels"
      description: "Number of EEG channels (up to 63, based on the number of channels selected)."
    - name: "n_timepoints"
      description: "Number of time points in the EEG epoch (up to 140, based on the number of time points selected)."


# Model parameters and their usage
parameters:
  subject:
    type: int
    required: true
    valid_values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    example: 1
    description: "Subject ID from the THINGS EEG2 dataset (1-10)."
    function: "get_encoding_model"

  selection:
    type: dict
    description: |
      Specifies which outputs to include in the model responses.
      Can include specific channels and/or timepoints. If not provided,
      EEG responses are generated for all EEG channels and time points.
    required: false
    function: get_encoding_model
    properties:
      channels:
        type: list[str]
        description: List of EEG channel names to include in the output
        valid_values: [
          'Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 
          'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 
          'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 
          'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz', 'F1', 'F5', 'FT7', 'FC3', 
          'FCz', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz', 
          'PO4', 'PO8', 'P6', 'P2', 'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4', 
          'FT8', 'F6', 'F2', 'AF4', 'AF8'
        ]
        example: ["Oz", "Cz", "Fp1"]
      timepoints:
        type: numpy.ndarray
        description: |
          Binary one-hot encoded vector indicating which timepoints to include.
          Must have exactly the same length as the number of available timepoints (140).
          Each position set to 1 indicates that timepoint should be included.
        example: [0, 0, ..., 1, 1, 0]  

  stimulus:
    type: numpy.ndarray
    required: true
    shape: [batch_size, 3, height, width]
    description: "A batch of RGB images to be encoded. Images should be in integer format with values in the range [0, 255], and square dimensions (e.g. 224×224)."
    example: "An array of shape [100, 3, 224, 224] representing 100 RGB images."
    function: "encode"

  device:
    type: str
    required: false
    valid_values: ["cpu", "cuda", "auto"]
    default: "auto"
    example: "auto"
    description: "Device to run the model on. 'auto' will use CUDA if available, otherwise CPU."
    function: "encode"
  
  show_progress:
    type: bool
    required: false
    default: True
    example: True
    description: "Whether to show a progress bar during encoding (for large batches)."
    function: "encode"

# Performance metrics and references
performance:
  
  accuracy_plots: 
    - "neural-encoding-simulation-toolkit/encoding_models/modality-eeg/train_dataset-things_eeg_2/model-vit_b_32/encoding_models_accuracy"
  
references:
    - Model building code: 'https://github.com/gifale95/NEST/tree/main/nest_creation_code'
    - THINGS EEG2 (Gifford et al., 2022): https://doi.org/10.1016/j.neuroimage.2022.119754
    - THINGS initiative (Hebart et al., 2019): https://things-initiative.org/
    - ViT-B/32 (Dosovitskiy et al., 2020): https://arxiv.org/abs/2010.11929
