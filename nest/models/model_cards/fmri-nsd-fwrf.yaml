model_id: fmri-nsd-fwrf
modality: fMRI
training_dataset: Natural Scenes Dataset (NSD)
species: Human # Add this
stimuli: Images # Add this
model_type: Feature-weighted receptive field (fwRF) # Model Architecture => Model type
creator: Alessandro Gifford

# General description of the model
description: |
  These encoding models consist in convolutional neural networks trained end-to-end to predict fMRI responses from input
  images using the feature-weighted receptive field (fwRF) (St-Yves & Naselaris, 2018).
  
  The encoding models were trained on the Natural Scenes Dataset (NSD) (Allen et al., 2022), 7T fMRI responses of 8
  subjects to 73k natural scenes coming from the COCO dataset (Lin et al., 2014). One encoding model was trained for
  each NSD subject, and for each of 23 ROIs overlaying visual cortex. For detailed information on these ROIs, and on how
  they were selected, refer to the NSD paper and data manual.
  
  **Preprocessing.** The encoding models are trained on NSD's subject-native volume data in “func1pt8mm” space, from the
  “betas_fithrf_GLMdenoise_RR” preprocessing version. Note that the NSD data were *z*-scored at each scan session, and as
  a consequence the in silico fMRI responses generated by the encoding models also live in *z*-scored space.
  
  **Model training partition.** fMRI responses for up to 9,000 non-shared images (i.e., the images uniquely seen by each
  subject during the NSD experiment).
  
  **Model validation partition.** fMRI responses for up to 485/1,000 shared images (i.e., the 485 shared images that not
  all subjects saw for up to three times during the NSD experiment).
  
  **Model testing partition.** fMRI responses for 515/1,000 shared images (i.e., the 515 images that each subject saw for
  exactly three times during the NSD experiment).

# Input stimulus information
input:
  type: "numpy.ndarray"
  shape: [batch_size, 3, height, width]
  description: "The input should be a batch of RGB images."
  constraints:
    - "Image values should be integers in range [0, 255]."
    - "Image dimensions (height, width) should be equal (square)."
    - "Minimum recommended image size: 224x224 pixels."

# Output information
output:
  type: "numpy.ndarray"
  shape: [batch_size, n_voxels]
  description: |
    The output is a 2D array containing in silico fMRI responses.
    The second dimension (n_voxels) corresponds to the number of voxels in the selected ROI,
    which varies by ROI and subject. For subject 1, the number of voxels per ROI is as follows:
    
      - V1: 1,350
      - V2: 1,433
      - V3: 1,187
      - hV4: 687
      - EBA: 2,971
      - FBA-2: 430
      - OFA: 355
      - FFA-1: 484
      - FFA-2: 310
      - PPA: 1,033
      - RSC: 566
      - OPA: 1,611
      - OWFA: 464
      - VWFA-1: 773
      - VWFA-2: 505
      - mfs-words: 165
      - early: 5,917
      - midventral: 986
      - midlateral: 834
      - midparietal: 950
      - parietal: 3,548
      - lateral: 7,799
      - ventral: 7,604
  dimensions:
    - name: "batch_size"
      description: "Number of stimuli in the batch"
    - name: "n_voxels"
      description: "Number of voxels in the selected ROI, varies by ROI and subject."

# Model parameters and their usage
parameters:
  subject:
    type: int
    required: true
    valid_values: [1, 2, 3, 4, 5, 6, 7, 8]
    example: 1
    description: "Subject ID from the NSD dataset (1-8)."
    function: "get_encoding_model"
    
  nest_dir:
    type: str
    required: false
    example: "./"
    description: "Root directory of the NEST repository (optional if default paths are set)."
    function: "get_encoding_model"

  selection:
    type: dict
    description: Specifies which outputs to include in the model responses.
    required: true
    function: get_encoding_model
    properties:
      roi:
        type: str
        description: |
          Region of Interest (ROI) for voxel prediction.
          Early visual areas (V1-V3), category-selective regions (EBA, FFA, etc.),
          or composite regions (lateral, ventral).
        valid_values: ["V1", "V2", "V3", "hV4", "EBA", "FBA-2", "OFA", "FFA-1", "FFA-2", "PPA",
        "RSC", "OPA", "OWFA", "VWFA-1", "VWFA-2", "mfs-words", "early",
        "midventral", "midlateral", "midparietal", "parietal", "lateral", "ventral"]
        example: "V1"

  stimulus:
    type: numpy.ndarray
    required: true
    shape: [batch_size, 3, height, width]
    description: "A batch of RGB images to be encoded. Images should be in integer format with values in the range [0, 255], and square dimensions (e.g. 224x224)."
    example: "An array of shape [100, 3, 224, 224] representing 100 RGB images."
    function: "encode"

    
  device:
    type: str
    required: false
    valid_values: ["cpu", "cuda", "auto"]
    default: "auto"
    example: "auto"
    description: "Device to run the model on. 'auto' will use CUDA if available, otherwise CPU."
    function: "encode"

# Performance metrics and references
performance:

  accuracy_plots: 
    - "neural_encoding_simulation_toolkit/encoding_models/modality-fmri/train_dataset-nsd/model-fwrf/encoding_models_accuracy"
  
references:
    - fwRF model (St-Yves et al., 2018): https://doi.org/10.1016/j.neuroimage.2017.06.035
    - NSD paper (Allen et al., 2022): https://doi.org/10.1038/s41593-021-00962-x
    - COCO dataset (Lin et al., 2014): https://cocodataset.org/#home
