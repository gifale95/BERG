===================
Available Models
===================

This page provides an overview of the brain encoding models currently available in NEST.


Model Naming Convention
----------------------

NEST contains several encoding models, defined by the following model ID naming convention:

``{modality}-{dataset}-{model}``

where

* ``modality``: The neural recording recording modality on which the encoding model was trained.
* ``dataset``: The neural dataset on which the encoding model was trained.
* ``model``: The type of encoding model used.

For example:

- ``fmri-nsd-fwrf``: An fMRI encoding model trained on the NSD using feature-weighted receptive fields.
- ``eeg-things_eeg_2-vit_b_32``: An EEG model trained on the THINGS-EEG2 dataset using the ViT-B/32 visual transformer architecture.


Get Model Information
------------------------

You can get detailed information about any model using:

.. code-block:: python

    from nest import NEST
    
    nest = NEST("path/to/neural_encoding_simulation_toolkit")

    # List all available models
    all_models = nest.list_models()
    
    # Get detailed model information
    model_info = nest.describe("fmri-nsd-fwrf")


Available models
----------------------

Following is a list of all available models, grouped by ``modality`` and ``dataset``.

modality-fmri
~~~~~~~~~~

Encoding models trained on neural responses recorded with functional Magnetic Resonance Imaging (fMRI).

dataset-nsd
^^^^^^^^^^^^^

Encoding models trained on the `Natural Scenes Dataset (NSD) <https://naturalscenesdataset.org/>`_ (`Allen et al., 2022 <https://doi.org/10.1038/s41593-021-00962-x>`_), 7T fMRI responses of 8 subjects to 73k natural scenes coming from the `COCO dataset <https://cocodataset.org/#home>`_ (`Lin et al., 2014 <https://doi.org/10.48550/arXiv.1405.0312>`_).
These models were trained on 23 ROIs overlaying visual cortex. For detailed information on these ROIs, and on how they were selected, refer to the NSD `paper <https://doi.org/10.1038/s41593-021-00962-x>`_ and `data manual <https://cvnlab.slite.page/p/X_7BBMgghj/ROIs>`_.

**Preprocessing.** Modeling is based on subject-native volume data in "func1pt8mm" space, from the "betas_fithrf_GLMdenoise_RR" preprocessing version. Note that the NSD data were *z*-scored at each scan session, and as a consequence the in silico fMRI responses generated by the encoding models also live in *z*-scored space.

**Model training partition.** fMRI responses for up to 9,000 non-shared images (i.e., the images uniquely seen by each subject during the NSD experiment).

**Model validation partition.** fMRI responses for up to 485/1,000 shared images (i.e., the 485 shared images that not all subjects saw for up to three times during the NSD experiment).

**Model testing partition.** fMRI responses for 515/1,000 shared images (i.e., the 515 images that each subject saw for exactly three times during the NSD experiment).

.. list-table::
   :header-rows: 1
   :widths: 20 55
   :class: wrap-table

   * - Model ID
     - Description
   * - :doc:`model_cards/fmri-nsd-fwrf`
     - Feature-weighted receptive fields, convolutional neural networks trained end-to-end to predict fMRI responses from input images.


modality-eeg
~~~~~~~~~~~~

Encoding models trained on neural responses recorded with functional Magnetic Resonance Imaging (Electroencephalography).

dataset-things_eeg_2
^^^^^^^^^^^^^^^^^^^^

Encoding models trained on `THINGS EEG2 <https://www.alegifford.com/projects/eeg_dataset/>`_ (`Gifford et al., 2022 <https://doi.org/10.1016/j.neuroimage.2022.119754>`_), 63-channel EEG responses of 10 subjects to over 16,740 images from the `THINGS initiative <https://things-initiative.org/>`_ (`Hebart et al., 2019 <https://doi.org/10.1371/journal.pone.0223792>`_).

**Preprocessing.** During preprocessing the 63-channel raw EEG data was filtered between 0.03 Hz and 100 Hz; epoched from -100 ms to +600 ms with respect to stimulus onset; transformed using current source density transform; downsampled to 200 Hz resulting in 140 times points per epoch (one every 5 ms); baseline corrected at each channel using the mean of the pre-stimulus interval. Note that, after preprocessing, the THINGS EEG2 data were *z-scored* at each scan session, and as a consequence the in silico EEG responses generated by the encoding models also live in *z*-scored space.

**Model training partition.** EEG responses for 16,540 unique images, each repeated 4 times (i.e., the official training partition of the THINGS EEG2 dataset).

**Model testing partition.** EEG responses for 200 unique images, each repeated 80 times (i.e., the official testing partition of the THINGS EEG2 dataset).

.. list-table::
   :header-rows: 1
   :widths: 20 55
   :class: wrap-table

   * - Model ID
     - Description
   * - :doc:`model_cards/eeg-things_eeg_2-vit_b_32`
     - Linear mapping of vision transformer image features onto EEG responses.

.. raw:: html

   <style>
   .wrap-table td {
     white-space: normal !important;
     word-wrap: break-word !important;
   }
   </style>

