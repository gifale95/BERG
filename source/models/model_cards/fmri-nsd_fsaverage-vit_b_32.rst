=============
fmri-nsd_fsaverage-vit_b_32
=============

Model Summary
------------

.. list-table::
   :widths: 30 70
   :stub-columns: 1

   * - Modality
     - fMRI
   * - Training Dataset
     - Natural Scenes Dataset (NSD) (fsaverage surface space)
   * - Species
     - Human
   * - Stimuli
     - Images
   * - Model Type
     - vision transformer (ViT-B/32)
   * - Creator
     - Alessandro Gifford

Description
----------

These encoding models consist in a linear mapping (through linear regression) of vision transformer
(Dosovitskiy et al., 2020) image features onto fMRI responses. Prior to mapping onto fMRI responses, the image features
have been downsampled to 250 principal components using principal component analysis.

The encoding models were trained on the Natural Scenes Dataset (NSD) (Allen et al., 2022), 7T fMRI responses of 8
subjects to 73k natural scenes coming from the COCO dataset (Lin et al., 2014). One encoding model was trained for
each NSD subject, and for each of 23 ROIs overlaying visual cortex. For detailed information on these ROIs, and on how
they were selected, refer to the NSD paper and data manual.

**Preprocessing.** The encoding models are trained on NSD's data prepared in FreeSurfer's fsaverage space, from the
"betas_fithrf_GLMdenoise_RR" preprocessing version. Note that the NSD data were *z*-scored at each scan session, and as
a consequence the in silico fMRI responses generated by the encoding models also live in *z*-scored space.

**Model training partition.** fMRI responses for up to 9,000 non-shared images (i.e., the images uniquely seen by each
subject during the NSD experiment).

**Model validation partition.** fMRI responses for up to 485/1,000 shared images (i.e., the 485 shared images that not
all subjects saw for up to three times during the NSD experiment).

**Model testing partition.** fMRI responses for 515/1,000 shared images (i.e., the 515 images that each subject saw for
exactly three times during the NSD experiment).

Input
-----

**Type**: ``numpy.ndarray``  
**Shape**: ``['batch_size', 3, 'height', 'width']``  
**Description**: The input should be a batch of RGB images.

**Constraints:**

* Image values should be integers in range [0, 255].
* Image dimensions (height, width) should be equal (square).
* Minimum recommended image size: 224Ã—224 pixels.

Output
------

**Type**: ``tuple of numpy.ndarray``  
**Shape**: ``([batch_size, lh_vertices], [batch_size, rh_vertices])``  
**Description**:  
The output is a tuple containing the left hemisphere (LH) and right hemisphere (RH) in silico fMRI responses for the batch
images.

**Dimensions:**

.. list-table::
   :widths: 30 70
   :header-rows: 1

   * - Name
     - Description
   * - batch_size
     - Number of stimuli in the batch
   * - lh_vertices
     - Number of selected LH vertices for which the in silico fMRI responses are generated.
   * - rh_vertices
     - Number of selected RH vertices for which the in silico fMRI responses are generated.

Parameters
---------

Parameters used in ``get_encoding_model``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This function loads the encoding model.

.. list-table::
   :widths: 20 80
   :header-rows: 0

   * - **subject**
     - | **Type:** int
       | **Required:** Yes
       | **Description:** Subject ID from the NSD dataset (1-8).
       | **Valid Values:** 1, 2, 3, 4, 5, 6, 7, 8
       | **Example:** 1
   * - **selection**
     - | **Type:** dict
       | **Required:** No
       | **Description:** Specifies which outputs to include in the model responses. If not provided, fMRI responses
       | are generate for all LH and RH fMRI vertices.
       | 
       | **Properties:**
       | 
       | **roi**
       |     **Type:** str
       |     **Description:**  The region-of-interest (ROI) for which the in silico fMRI responses (of both
       |     hemispherese) are generated.
       |     **Valid values:** "V1d", "V1v", "V2d", "V2v", "V3d", "V3v", "hV4", "OFA", "FFA-1", "FFA-2", "mTL-faces", "aTL-faces", "OVWFA", "VWFA-1", "VWFA-2", "mfs-words", "mTL-words", "OPA","PPA","RSC","EBA","FBA-1","FBA-2","mTL-bodies","early", "midventral", "midlateral", "midparietal", "parietal", "lateral", "ventral"
       |     **Example:** "V1v"
       |
       | **lh_vertices**
       |     **Type:** numpy.ndarray
       |     **Description:**  Binary one-hot encoded vector with ones indicating the left hemisphere (LH) vertices for
       |     which the in silico fMRI responses are generated. This vector must have exactly the same length as the number
       |     of LH fsaverage vertices (163,842). The vertices from the one-hot encoded vector are only selected if the "roi"
       |     key is not provided, or has value None.
       |     **Example:** [0, 0, ..., 1, 1, 0]
       |
       | **rh_vertices**
       |     **Type:** numpy.ndarray
       |     **Description:**  Binary one-hot encoded vector with ones indicating the right hemisphere (RH) vertices for
       |     which the in silico fMRI responses are generated. This vector must have exactly the same length as the number
       |     of RH fsaverage vertices (163,842). The vertices from the one-hot encoded vector are only selected if the "roi"
       |     key is not provided, or has value None.
       |     **Example:** [0, 1, ..., 1, 0, 0]

Parameters used in ``encode``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This function generates in silico neural responses using the encoding model previously loaded.

.. list-table::
   :widths: 20 80
   :header-rows: 0

   * - **stimulus**
     - | **Type:** numpy.ndarray
       | **Required:** Yes
       | **Description:** A batch of RGB images to be encoded. Images should be in integer format with values in the range [0, 255], and square dimensions (e.g. 224x224).
       | **Example:** An array of shape [100, 3, 224, 224] representing 100 RGB images.
   * - **device**
     - | **Type:** str
       | **Required:** No
       | **Description:** Device to run the model on. 'auto' will use CUDA if available, otherwise CPU.
       | **Valid Values:** "cpu", "cuda", "auto"
       | **Example:** "auto"

Performance
----------

**Accuracy Plots:**

* ``neural-encoding-simulation-toolkit/encoding_models/modality-fmri/train_dataset-nsd_fsaverage/model-vit_b_32/encoding_models_accuracy``

Example Usage
------------

.. code-block:: python

    from berg import BERG
    
    # Initialize BERG
    berg = BERG(berg_dir="path/to/brain-encoding-response-generator")
    
    # Load the encoding model
    model = berg.get_encoding_model(
      "fmri-nsd_fsaverage-vit_b_32",
      subject=1,
      selection={
        "roi": "V1v"
      }
    )
    
    # Prepare the stimulus images
    # Image shape should be [batch_size, 3 RGB channels, height, width]
    images = np.random.randint(0, 255, (100, 3, 256, 256))
    
    # Generates the in silico neural responses to images using the encoding model previously loaded
    responses = berg.encode(
      model,
      images,
      device="auto",
      show_progress=True
    )
    
    # The in silico fMRI responses will be a tuple of numpy.ndarray of shape: 
    # ([batch_size, lh_vertices], [batch_size, rh_vertices])
    # where:
    # - lh_vertices is the number of selected left hemisphere (LH) vertices for which the in silico
    # fMRI responses are generated."
    # - rh_vertices is the number of selected right hemisphere (RH) vertices for which the in silico
    # fMRI responses are generated."

    # Generate in silico neural responses with metadata
    responses, metadata = berg.encode(model, images, return_metadata=True)

References
---------

* {'Model building code': 'https://github.com/gifale95/BERG/tree/main/berg_creation_code'}
* {'NSD paper (Allen et al., 2022)': 'https://doi.org/10.1038/s41593-021-00962-x'}
* {'COCO dataset (Lin et al., 2014)': 'https://cocodataset.org/#home'}
* {'ViT-B/32 (Dosovitskiy et al., 2020)': 'https://arxiv.org/abs/2010.11929'}
