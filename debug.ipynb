{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"G4ze_dIrxRN_"},"outputs":[],"source":["import h5py\n","import matplotlib\n","from matplotlib import pyplot as plt\n","from berg import BERG\n","import nibabel as nib\n","import numpy as np\n","import os\n","from PIL import Image\n","import torchvision\n","from torchvision import transforms as trn\n","from tqdm import tqdm\n","from IPython.display import display, JSON\n","\n","berg_dir = \"/Volumes/Extreme SSD/Datasets/neural-encoding-simulation-toolkit\" \n","test_images = \"tutorial_images\""]},{"cell_type":"markdown","metadata":{"id":"oJMw5lncBZMX"},"source":["# 1 | Load and Prepare the Images for Generating the In Silico Neural Responses\n","\n","With the following code you will load and preprocess the images that you will later use to generate the in silico neural responses using an encoding model of your choice.\n","\n","These encoding model expects as input images in a specific format: a 4D numpy array with shape (Batch size Ã— 3 RGB Channels Ã— Width Ã— Height) and integer values in the range [0, 255]. Because the images must also be of square size (equal width and height), you will center crop them during loading."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":640,"status":"ok","timestamp":1747391537955,"user":{"displayName":"Alessandro Gifford","userId":"14433410180935884017"},"user_tz":-120},"id":"6RY2YSN_Bw8v","outputId":"ca15b97f-5aed-4335-80e4-47107782306e"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 324.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Images shape:\n","(100, 3, 227, 227)\n","(Batch size Ã— 3 RGB Channels x Width x Height)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["images_dir = test_images\n","images_list = os.listdir(images_dir)\n","images_list.sort()\n","\n","images = []\n","for img in tqdm(images_list):\n","    img_dir = os.path.join(images_dir, img)\n","    img = Image.open(img_dir).convert('RGB')\n","    # Center crop the images to square format, and resize them\n","    transform = trn.Compose([\n","        trn.CenterCrop(min(img.size)),\n","        trn.Resize((227,227))\n","    ])\n","    img = transform(img)\n","    img = np.asarray(img)\n","    img = img.transpose(2,0,1)\n","    images.append(img)\n","images = np.asarray(images)\n","\n","# Print the images dimensions\n","print('\\n\\nImages shape:')\n","print(images.shape)\n","print('(Batch size Ã— 3 RGB Channels x Width x Height)')"]},{"cell_type":"markdown","metadata":{"id":"PSbMw8SUU4fw"},"source":["## 2.1 | Create the BERG object\n","\n","To use the BERG package, you first need to create a `BERG` object, by providing the path to the Neural Encoding Simulation Toolkit directory. This object will be the instance through which you can generate in silico neural responses."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1o8VlO5vQT0E"},"outputs":[],"source":["# Initialize the BERG object with the path to the toolkit directory\n","berg = BERG(berg_dir)"]},{"cell_type":"markdown","metadata":{"id":"HsMrdxx79PBU"},"source":["## 2.2 | Browse the available encoding models\n","\n","The `list_models()` method lists the encoding models that are available in BERG. BERG contains several encoding models, defined by the following model ID naming convention:\n","\n","`{modality}-{dataset}-{model}`\n","\n","where\n","\n","* **`modality`:** The neural recording recording modality on which the encoding model was trained.\n","* **`dataset`:** The neural dataset on which the encoding model was trained.\n","* **`model`:** The type of encoding model used.\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1747391790991,"user":{"displayName":"Alessandro Gifford","userId":"14433410180935884017"},"user_tz":-120},"id":"qNOlnoBg9PBU","outputId":"b0a1b5cc-05b3-4dc8-a71b-9f350af140b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Available models: ['fmri-nsd-fwrf', 'fmri-nsd_fsaverage-vit_b_32', 'eeg-things_eeg_2-vit_b_32']\n"]}],"source":["# List all available models and their versions\n","available_models = berg.list_models()\n","print(f\"Available models: {available_models}\")"]},{"cell_type":"markdown","metadata":{"id":"4__DFvu29PBU"},"source":["You can also view the modalities and datasets in a more structured format:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1747391800072,"user":{"displayName":"Alessandro Gifford","userId":"14433410180935884017"},"user_tz":-120},"id":"U6_dWaWC9PBU","outputId":"fb2994b7-3836-4bb4-8467-238ad72e151e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Available Modalities and Datasets:\n","=================================\n","â€¢ EEG\n","  â””â”€ THINGS EEG2\n","\n","â€¢ FMRI\n","  â””â”€ Natural Scenes Dataset (NSD) (fsaverage surface space)\n","  â””â”€ Natural Scenes Dataset (NSD) (subject-native volume space)\n","\n","Model Catalog as Dict: {'fMRI': ['Natural Scenes Dataset (NSD) (fsaverage surface space)', 'Natural Scenes Dataset (NSD) (subject-native volume space)'], 'EEG': ['THINGS EEG2']}\n"]}],"source":["# Get a hierarchical view of available models by modality and dataset\n","catalog = berg.get_model_catalog(print_format=True)\n","print(f\"Model Catalog as Dict: {catalog}\")"]},{"cell_type":"markdown","metadata":{"id":"JyHCEKUQ9PBU"},"source":["The `print_format=True` parameter displays a nicely formatted hierarchical view, making it easy to browse the available encoding models.\n","\n","<font color='red'><b>NOTE:</b></font> For a list of all available encoding model, please see the [documentation](https://neural-encoding-simulation-toolkit.readthedocs.io/en/latest/models/overview.html)."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"MPa-HNaA-D25"},"outputs":[],"source":["# Select one of the existing models\n","model_id = 'eeg-things_eeg_2-vit_b_32'  #@param [\"fmri-nsd-fwrf\", \"eeg-things_eeg_2-vit_b_32\"]"]},{"cell_type":"markdown","metadata":{"id":"W3vG9_U69PBU"},"source":["## 2.3 | Get detailed model information\n","\n","With the `describe()` you can get detailed information about this model and how to use it, including:\n","\n","* **Basic Information:** Details about the modality, dataset, and encoding model.\n","* **Description:** How the model works and what it does.\n","* **Input Requirements:** Format specifications for input stimuli (dimensions, type, etc.).\n","* **Output Format:** The structure and meaning of the model's in silico neural predictions.\n","* **Parameters:** Required and optional arguments for the model's functions.\n","* **Performance Information:** Directory where to find the encodind model's prediction accuracy plots.\n","* **Usage Examples:** Code snippets showing how to use the model.\n","\n","This information will help you understand how to properly set up and use the model in the following sections.\n","\n","<font color='red'><b>NOTE:</b></font> You can also view this information on the model in the [documentation](https://neural-encoding-simulation-toolkit.readthedocs.io/en/latest/models/overview.html)."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1747230962591,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"H3E-4f339PBV","outputId":"eb5ec8d3-9b3c-4772-df6b-17ee056b0ecb"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","ðŸ§  Model: eeg-things_eeg_2-vit_b_32\n","================================================================================\n","\n","Modality: EEG\n","Training dataset: THINGS EEG2\n","Creator: Alessandro Gifford\n","\n","ðŸ“‹ Description:\n","These encoding models consist in a linear mapping (through linear regression) of\n","vision transformer (Dosovitskiy et al., 2020) image features onto EEG responses.\n","Prior to mapping onto EEG responses, the image features have been downsampled to\n","250 principal components using principal component analysis.  The encoding\n","models were trained on THINGS EEG2 (Gifford et al., 2022), 63-channel EEG\n","responses of 10 subjects to over 16,740 images from the THINGS initiative\n","(Hebart et al., 2019).  **Preprocessing**. During preprocessing the 63-channel\n","raw EEG data was filtered between 0.03 Hz and 100 Hz; epoched from -100 ms to\n","+600 ms with respect to stimulus onset; transformed using current source density\n","transform; downsampled to 200 Hz resulting in 140 times points per epoch (one\n","every 5 ms); baseline corrected at each channel using the mean of the pre-\n","stimulus interval.  **Model training partition.** EEG responses for 16,540\n","unique images, each repeated 4 times (i.e., the official training partition of\n","the THINGS EEG2 dataset).  **Model testing partition.** EEG responses for 200\n","unique images, each repeated 80 times (i.e., the official testing partition of\n","the THINGS EEG2 dataset).  Independent encoding models were trained for each of\n","the 4 training data repeats, and as a result the trained encoding models\n","generate 4 instances (i.e., repeats) of in silico EEG responses.  Indepedent\n","encoding models were trained for each subject, channel, and time point.\n","\n","ðŸ“¥ Input:\n","  Type: numpy.ndarray\n","  Shape: ['batch_size', 3, 'height', 'width']\n","  Description: The input should be a batch of RGB images.\n","  Constraints:\n","    â€¢ Image values should be integers in range [0, 255].\n","    â€¢ Image dimensions (height, width) should be equal (square).\n","    â€¢ Minimum recommended image size: 224Ã—224 pixels.\n","\n","ðŸ“¤ Output:\n","  Type: numpy.ndarray\n","  Shape: ['batch_size', 'n_repetitions', 'n_channels', 'n_timepoints']\n","  Description: The output is a 4D array containing in silico EEG responses.\n","  Dimensions:\n","    â€¢ batch_size: Number of stimuli in the batch.\n","    â€¢ n_repetitions: Number of simulated repetitions of the same stimulus (always 4).\n","    â€¢ n_channels: Number of EEG channels (up to 63, based on the number of channels selected).\n","    â€¢ n_timepoints: Number of time points in the EEG epoch (up to 140, based on the number of time points selected).\n","\n","ðŸ“Œ Parameters for encode():\n","\n","â€¢ stimulus (numpy.ndarray, required)\n","  â†³ A batch of RGB images to be encoded. Images should be in integer format with\n","    values in the range [0, 255], and square dimensions (e.g. 224Ã—224).\n","\n","â€¢ device (str, optional, default='auto')\n","  â†³ Device to run the model on. 'auto' will use CUDA if available, otherwise\n","    CPU.\n","  â†³ Valid values: ['cpu', 'cuda', 'auto']\n","\n","â€¢ show_progress (bool, optional, default=True)\n","  â†³ Whether to show a progress bar during encoding (for large batches).\n","\n","ðŸ“Œ Parameters for get_encoding_model():\n","\n","â€¢ subject (int, required)\n","  â†³ Subject ID from the THINGS EEG2 dataset (1-10).\n","  â†³ Valid values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","\n","â€¢ selection (dict, optional)\n","  â†³ Specifies which outputs to include in the model responses. Can include\n","    specific channels and/or timepoints. If not provided, EEG responses are\n","    generated for all EEG channels and time points.\n","\n","  â†ª Sub-parameters within 'selection':\n","\n","    â€¢ channels (list[str])\n","      â†³ List of EEG channel names to include in the output\n","      â†³ Valid values: 'Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz', 'F1', 'F5', 'FT7', 'FC3', 'FCz', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'P6', 'P2', 'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4', 'FT8', 'F6', 'F2', 'AF4', 'AF8']\n","      â†³ Example: ['Oz', 'Cz', 'Fp1']\n","\n","    â€¢ timepoints (numpy.ndarray)\n","      â†³ Binary one-hot encoded vector indicating which timepoints to include.\n","        Must have exactly the same length as the number of available timepoints\n","        (140). Each position set to 1 indicates that timepoint should be\n","        included.\n","      â†³ Example: [0, 0, '...', 1, 1, 0]\n","\n","ðŸ“Š Performance:\n","\n","  Performance plots at: brain-encoding-response-generator/encoding_models/modality-eeg/train_dataset-things_eeg_2/model-vit_b_32/encoding_models_accuracy\n","\n","ðŸ“š References:\n","    â€¢ {'Model building code':\n","      'https://github.com/gifale95/BERG/tree/main/berg_creation_code'}\n","    â€¢ {'THINGS EEG2 (Gifford et al., 2022)':\n","      'https://doi.org/10.1016/j.neuroimage.2022.119754'}\n","    â€¢ {'THINGS initiative (Hebart et al., 2019)': 'https://things-\n","      initiative.org/'}\n","    â€¢ {'ViT-B/32 (Dosovitskiy et al., 2020)':\n","      'https://arxiv.org/abs/2010.11929'}\n","\n","ðŸ“¦ Example Usage:\n","\n","from berg import BERG\n","\n","berg = BERG(\"path/to/neural_encoding_simulation_toolkit\")\n","\n","# Initialize the model\n","model = berg.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", subject=1, selection={'channels': ['Oz', 'Cz', 'Fp1'], 'timepoints': [0, 0, '...', 1, 1, 0]})\n","\n","# Generate responses (assuming stimulus is a numpy array)\n","responses = model.generate_response(stimulus)\n","\n","================================================================================\n","['model_id', 'model_info', 'description', 'input', 'output', 'parameters', 'parameters_by_function', 'performance', 'example_usage']\n"]}],"source":["# Get comprehensive information about the fMRI model\n","model_info = berg.describe(model_id)\n","print(list(model_info.keys()))"]},{"cell_type":"markdown","metadata":{"id":"drLmqmnBAKTf"},"source":["You can examine the parameters required for your specific model:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1747230962600,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"Vu-fAy5iAq0e","outputId":"55aeec6c-2c47-46d4-d2c2-bbdda812db23"},"outputs":[{"data":{"application/json":{"device":{"default":"auto","description":"Device to run the model on. 'auto' will use CUDA if available, otherwise CPU.","example":"auto","function":"encode","required":false,"type":"str","valid_values":["cpu","cuda","auto"]},"selection":{"description":"Specifies which outputs to include in the model responses.\nCan include specific channels and/or timepoints. If not provided,\nEEG responses are generated for all EEG channels and time points.\n","function":"get_encoding_model","properties":{"channels":{"description":"List of EEG channel names to include in the output","example":["Oz","Cz","Fp1"],"type":"list[str]","valid_values":["Fp1","F3","F7","FT9","FC5","FC1","C3","T7","TP9","CP5","CP1","Pz","P3","P7","O1","Oz","O2","P4","P8","TP10","CP6","CP2","Cz","C4","T8","FT10","FC6","FC2","F4","F8","Fp2","AF7","AF3","AFz","F1","F5","FT7","FC3","FCz","C1","C5","TP7","CP3","P1","P5","PO7","PO3","POz","PO4","PO8","P6","P2","CPz","CP4","TP8","C6","C2","FC4","FT8","F6","F2","AF4","AF8"]},"timepoints":{"description":"Binary one-hot encoded vector indicating which timepoints to include.\nMust have exactly the same length as the number of available timepoints (140).\nEach position set to 1 indicates that timepoint should be included.\n","example":[0,0,"...",1,1,0],"type":"numpy.ndarray"}},"required":false,"type":"dict"},"show_progress":{"default":true,"description":"Whether to show a progress bar during encoding (for large batches).","example":true,"function":"encode","required":false,"type":"bool"},"stimulus":{"description":"A batch of RGB images to be encoded. Images should be in integer format with values in the range [0, 255], and square dimensions (e.g. 224Ã—224).","example":"An array of shape [100, 3, 224, 224] representing 100 RGB images.","function":"encode","required":true,"shape":["batch_size",3,"height","width"],"type":"numpy.ndarray"},"subject":{"description":"Subject ID from the THINGS EEG2 dataset (1-10).","example":1,"function":"get_encoding_model","required":true,"type":"int","valid_values":[1,2,3,4,5,6,7,8,9,10]}},"text/plain":["<IPython.core.display.JSON object>"]},"metadata":{"application/json":{"expanded":false,"root":"root"}},"output_type":"display_data"}],"source":["display(JSON(model_info[\"parameters\"]))"]},{"cell_type":"markdown","metadata":{"id":"g8-qSNfe9PBV"},"source":["---\n","---"]},{"cell_type":"markdown","metadata":{"id":"zr0YWe4UPJVQ"},"source":["\n","# 3 | The BERG Workflow: Generate In Silico Neural Responses\n","\n","BERG has a consistent workflow across different neural modalities and models. This workflow comes down to just **two key functions**:\n","\n","- `get_encoding_model()` â€“ Load an encoding model of your choice.\n","- `encode()` â€“ Generate in silico neural responses using the encoding model previously loaded.\n","\n","<font color='red'><b>NOTE:</b></font> For a detailed description of these functions' input parameters and outputs, please see the [documentation](https://neural-encoding-simulation-toolkit.readthedocs.io/en/latest/models/overview.html)."]},{"cell_type":"markdown","metadata":{"id":"Z10obx3KPsHs"},"source":["## 3.1 | Load the encoding models with get_encoding_model()\n","\n","You will first load your chosen model using `get_encoding_model()`:\n","\n","```python\n","# General format\n","model = berg.get_encoding_model(model_id, subject, **other_parameters)\n","```\n","\n","### Key parameters:\n","- `model_id`: The identifier for the model (e.g., \"fmri-nsd-fwrf\").\n","- `subject`: Subject ID ib which the encoding model was trained (required for all models).\n","- `device`: Computing device (\"cpu\", \"cuda\", or \"auto\").\n","- `selection`: Optional dictionary to specify the neural features for which the in silico neural responses are generated (e.g., specific fMRI ROIs or voxels, or specific EEG channels or time points)."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6517,"status":"ok","timestamp":1747230969120,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"xxFRfvyofmDR","outputId":"7f26a33f-9bc6-4a3c-d6fc-9d551e500266"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded on cpu for subject 1\n"]}],"source":["# Load the selected model for subject 1\n","model = berg.get_encoding_model(model_id,\n","                               subject=1,\n","                               device=\"auto\")\n","\n","# # If you are using the 'fmri-nsd-fwrf' encoding model, you should select the\n","# # for which you want to generate the in silico fMRI responses.\n","# model = berg.get_encoding_model(model_id,\n","#                                 subject=1,\n","#                                 selection={\"roi\": \"V1\"}, # or other\n","#                                 device=\"cpu\")\n","\n","# # If you are using the 'fmri-nsd-fwrf' encoding model, you can select the\n","# # EEG channels and/or time points for which you want to generate the in silico\n","# # EEG responses.\n","# timepoint_mask = [1]*48 + [0]*92\n","# model = berg.get_encoding_model(model_id,\n","#                                 subject=1,\n","#                                 selection={\n","#                                     \"channels\": [\"F7\"],\n","#                                     \"timepoints\": timepoint_mask}\n","#                                 )"]},{"cell_type":"markdown","metadata":{"id":"bepd25VkWHV_"},"source":["## 3.2 | Generate the in silico neural responses with encode()\n","\n","Once the model is loaded, you can use it to generate in silico neural responses to images using the `encode()` function:\n","\n","```python\n","# Basic usage\n","responses = berg.encode(model, images)\n","\n","# To also get metadata relative to the in silico neural responses\n","responses, metadata = berg.encode(model, images, return_metadata=True)\n","```"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2426,"status":"ok","timestamp":1747230971542,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"wfcKFjojgqP_","outputId":"631bbb36-e028-4313-cd2f-ee0a4b8094fc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Encoding EEG responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.52s/it, Encoded images=100, Total images=100]\n"]}],"source":["# Generate in silico neural responses for images\n","in_silico_responses = berg.encode(model, images)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1747230971543,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"1nTpHRn_s9ox","outputId":"10fb361e-7afe-4174-a25e-4338e3e42653"},"outputs":[{"name":"stdout","output_type":"stream","text":["Response shape: (100, 4, 63, 140)\n"]}],"source":["print(f\"Response shape: {in_silico_responses.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"ezCWXCNkgzZQ"},"source":["## 3.3 | Load the metadata\n","\n","BERG provides metadata for each model, which includes information on the neural data used to train the encoding models, and on the trained models themselves.\n","\n","You can access this metadata in two ways:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1613,"status":"ok","timestamp":1747231621243,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"9KivyH8ng3Ho","outputId":"987dd5d1-852c-4dc2-bf09-d0a42d4d6826"},"outputs":[{"name":"stderr","output_type":"stream","text":["Encoding EEG responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.52s/it, Encoded images=100, Total images=100]\n"]}],"source":["# Method 1: Get metadata during encoding\n","in_silico_responses, metadata = berg.encode(model, images, return_metadata=True)\n","\n","# Method 2: Get metadata directly from the encoding model with selection parameters\n","example_model = \"fmri-nsd-fwrf\"\n","example_subject = 1\n","example_roi = \"V1\"\n","metadata = berg.get_model_metadata(example_model, subject=example_subject, roi=example_roi)"]},{"cell_type":"markdown","metadata":{"id":"g2RU4znghb1T"},"source":["---\n","---\n","# 4 | Complete Working Examples"]},{"cell_type":"markdown","metadata":{"id":"cAEmXgvLhknK"},"source":["## Example 1: fMRI ROI selection\n","\n","In this example you will use the `fmri-nsd-fwrf` encoding model."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"DJphQ-5ljwE_"},"outputs":[],"source":["#@title Select the fMRI encoding model's subject and ROI\n","subject = 1  #@param {type:\"slider\", min:1, max:8, step:1}\n","roi = \"V1\"  #@param [\"V1\", \"V2\", \"V3\", \"hV4\", \"EBA\", \"FBA-2\", \"OFA\", \"FFA-1\", \"FFA-2\", \"PPA\", \"RSC\", \"OPA\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"parietal\", \"lateral\", \"ventral\"]\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1093,"status":"ok","timestamp":1747231622340,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"n-lHEz_ohhGW","outputId":"7fd41763-34aa-40cb-9a2d-10d9a40b66a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded on cpu for subject 1, ROI V1\n"]},{"name":"stderr","output_type":"stream","text":["Encoding fMRI responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s, Encoded images=100, Total images=100]"]},{"name":"stdout","output_type":"stream","text":["V1 responses shape: (100, 1350)\n","\n","fMRI response shape: (100, 1350)\n","dict_keys(['fmri', 'encoding_models'])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Initialize BERG\n","from berg import BERG\n","berg = BERG(berg_dir)\n","\n","# Load the encoding model of the selected ROI and subject\n","fmri_model = berg.get_encoding_model(\"fmri-nsd-fwrf\",\n","                                subject=subject,\n","                                selection={\"roi\": roi})\n","\n","# Generate the in silico fMRI responses\n","insilico_fmri = berg.encode(fmri_model, images)\n","print(f\"{roi} responses shape: {insilico_fmri.shape}\")\n","\n","# Print the in silico fMRI responses shape\n","import sys; sys.stdout.write(\"\\n\")\n","print(f\"fMRI response shape: {insilico_fmri.shape}\")\n","\n","# Show metadata\n","metadata = berg.get_model_metadata(\"fmri-nsd-fwrf\", subject=subject, roi=roi)\n","print(metadata.keys())"]},{"cell_type":"markdown","metadata":{"id":"z8F9uvcVhmWA"},"source":["## Example 2: EEG channel and time point selection\n","\n","In this example you will use the `eeg-things_eeg_2-vit_b_32` encoding model."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"E9K5q4EgkWrc"},"outputs":[],"source":["#@title Select EEG enconding model's subject, channels and time points\n","\n","#@markdown Select Subject\n","subject = 1  #@param {type:\"slider\", min:1, max:10, step:1}\n","\n","#@markdown Select the used EEG channels\n","# ['Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9',\n","# 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8',\n","# 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2',\n","# 'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz', 'F1', 'F5', 'FT7', 'FC3',\n","# 'FCz', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz',\n","# 'PO4', 'PO8', 'P6', 'P2', 'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4',\n","# 'FT8', 'F6', 'F2', 'AF4', 'AF8']\n","\n","channels = ['F7', 'F3', 'F4', 'F8']  #@param {type:\"raw\"}\n","\n","#@markdown Select the EEG time point range (between 0 and 140 timepoints)\n","start_timepoint = 0  #@param {type:\"slider\", min:0, max:139, step:1}\n","end_timepoint = 140  #@param {type:\"slider\", min:1, max:140, step:1}\n","\n","# Create timepoint mask from start and end selection\n","timepoint_mask = [1 if start_timepoint <= i < end_timepoint else 0 for i in range(140)]\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3101,"status":"ok","timestamp":1747231633950,"user":{"displayName":"Domenic Be","userId":"12291498344402974224"},"user_tz":-120},"id":"_93azNlzhp54","outputId":"ed20cb6b-4758-49a0-ee4f-81b2857cca91"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded on cpu for subject 1\n"]},{"name":"stderr","output_type":"stream","text":["Encoding EEG responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.45s/it, Encoded images=100, Total images=100]"]},{"name":"stdout","output_type":"stream","text":["\n","EEG response shape: (100, 4, 4, 140)\n","dict_keys(['eeg', 'encoding_models'])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from berg import BERG\n","\n","# Initialize BERG\n","berg = BERG(berg_dir)\n","\n","# Load the EEG encoiding model\n","eeg_model = berg.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\",\n","                                    subject=subject,\n","                                    selection={\n","                                        \"channels\": channels,\n","                                        \"timepoints\": timepoint_mask\n","                                    })\n","\n","# Generate the in silico EEG responses\n","insilico_eeg = berg.encode(eeg_model, images)\n","\n","# Print the in silico EEG responses shape\n","import sys; sys.stdout.write(\"\\n\")\n","print(f\"EEG response shape: {insilico_eeg.shape}\")\n","\n","# Show metadata\n","metadata = berg.get_model_metadata(\"eeg-things_eeg_2-vit_b_32\", subject=subject)\n","print(metadata.keys())"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}
