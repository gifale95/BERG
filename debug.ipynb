{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_dir = \"neural_encoding_simulation_toolkit\" \n",
    "test_images = \"tutorial_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 322.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Images shape:\n",
      "(100, 3, 227, 227)\n",
      "(Batch size Ã— 3 RGB Channels x Width x Height)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import transforms as trn\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "images_dir = os.path.join(test_images)\n",
    "images_list = os.listdir(images_dir)\n",
    "images_list.sort()\n",
    "    \n",
    "images = []\n",
    "for img in tqdm(images_list):\n",
    "    img_dir = os.path.join(images_dir, img)\n",
    "    img = Image.open(img_dir).convert('RGB')\n",
    "    # Center crop the images to square format, and resize them\n",
    "    transform = trn.Compose([\n",
    "        trn.CenterCrop(min(img.size)),\n",
    "        trn.Resize((227,227))\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = np.asarray(img)\n",
    "    img = img.transpose(2,0,1)\n",
    "    images.append(img)\n",
    "images = np.asarray(images)\n",
    "\n",
    "# Print the images dimensions\n",
    "print('\\n\\nImages shape:')\n",
    "print(images.shape)\n",
    "print('(Batch size Ã— 3 RGB Channels x Width x Height)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the code in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['fmri-nsd-fwrf', 'eeg-things_eeg_2-vit_b_32']\n",
      "Available Modalities and Datasets:\n",
      "=================================\n",
      "â€¢ EEG\n",
      "  â””â”€ things_eeg_2\n",
      "\n",
      "â€¢ FMRI\n",
      "  â””â”€ nsd\n",
      "\n",
      "Model Catalog as Dict:: {'fmri': ['nsd'], 'eeg': ['things_eeg_2']}\n"
     ]
    }
   ],
   "source": [
    "from nest import NEST\n",
    "\n",
    "nest = NEST(nest_dir=nest_dir)\n",
    "\n",
    "# List all available models and their versions\n",
    "available_models = nest.list_models()\n",
    "print(f\"Available models: {available_models}\")\n",
    "\n",
    "# See what modalities are available\n",
    "catalog = nest.get_model_catalog(print_format=True)\n",
    "print(f\"Model Catalog as Dict:: {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1, ROI V1\n",
      "Model loaded on cpu for subject 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding fMRI responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.26it/s, Encoded images=100, Total images=100]\n",
      "Encoding EEG responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.73s/it, Encoded images=100, Total images=100]\n"
     ]
    }
   ],
   "source": [
    "# Load Encoding Model\n",
    "fwrf_model = nest.get_encoding_model(\"fmri-nsd-fwrf\", \n",
    "                                     subject=1, \n",
    "                                     device=\"cpu\",\n",
    "                                     selection={\"roi\": \"V1\"})\n",
    "\n",
    "\n",
    "eeg_model = nest.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", \n",
    "                                    subject=1,\n",
    "                                    device=\"auto\")\n",
    "\n",
    "\n",
    "fwrf_silico = nest.encode(fwrf_model, images)\n",
    "eeg_silico, eeg_metadata = nest.encode(eeg_model, images,return_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fwRF Model in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§  Model: fmri-nsd-fwrf\n",
      "================================================================================\n",
      "\n",
      "Modality: fmri\n",
      "Training dataset: NSD\n",
      "Model architecture: feature-weighted receptive fields (fwrf)\n",
      "Creator: Alessandro Gifford\n",
      "\n",
      "ðŸ“‹ Description:\n",
      "This model generates in silico fMRI responses to visual stimuli using feature-\n",
      "weighted receptive fields (fwrf). It was trained on the Natural Scenes Dataset\n",
      "(NSD), a large-scale 7T fMRI dataset of subjects viewing natural images. The\n",
      "model extracts visual features using a convolutional neural network and maps\n",
      "these features to brain activity  patterns across multiple visual regions of\n",
      "interest (ROIs).  The model takes as input a batch of RGB images in the shape\n",
      "[batch_size, 3, height, width], with pixel values ranging from 0 to 255 and\n",
      "square dimensions (e.g., 224Ã—224).\n",
      "\n",
      "ðŸ“¥ Input:\n",
      "  Type: numpy.ndarray\n",
      "  Shape: ['batch_size', 3, 'height', 'width']\n",
      "  Description: The input should be a batch of RGB images.\n",
      "  Constraints:\n",
      "    â€¢ Image values should be integers in range [0, 255]\n",
      "    â€¢ Image dimensions (height, width) should be equal (square)\n",
      "    â€¢ Minimum recommended image size: 224x224 pixels\n",
      "\n",
      "ðŸ“¤ Output:\n",
      "  Type: numpy.ndarray\n",
      "  Shape: ['batch_size', 'n_voxels']\n",
      "  Description: The output is a 2D array containing predicted fMRI responses.\n",
      "The second dimension (n_voxels) corresponds to the number of voxels in the selected ROI,\n",
      "which varies by ROI and subject. For subject 1, the number of voxels per ROI is as follows:\n",
      "\n",
      "  - V1: 1350\n",
      "  - V2: 1433\n",
      "  - V3: 1187\n",
      "  - hV4: 687\n",
      "  - EBA: 2971\n",
      "  - FBA-2: 430\n",
      "  - OFA: 355\n",
      "  - FFA-1: 484\n",
      "  - FFA-2: 310\n",
      "  - PPA: 1033\n",
      "  - RSC: 566\n",
      "  - OPA: 1611\n",
      "  - OWFA: 464\n",
      "  - VWFA-1: 773\n",
      "  - VWFA-2: 505\n",
      "  - mfs-words: 165\n",
      "  - early: 5917\n",
      "  - midventral: 986\n",
      "  - midlateral: 834\n",
      "  - midparietal: 950\n",
      "  - parietal: 3548\n",
      "  - lateral: 7799\n",
      "  - ventral: 7604\n",
      "\n",
      "  Dimensions:\n",
      "    â€¢ batch_size: Number of stimuli in the batch\n",
      "    â€¢ n_voxels: Number of voxels in the selected ROI, varies by ROI and subject\n",
      "\n",
      "ðŸ“Œ Parameters for encode():\n",
      "\n",
      "â€¢ stimulus (numpy.ndarray, required)\n",
      "  â†³ A batch of RGB images to be encoded. Images should be in integer format with\n",
      "    values in the range [0, 255], and square dimensions (e.g. 224x224).\n",
      "\n",
      "â€¢ device (str, optional, default='auto')\n",
      "  â†³ Device to run the model on. 'auto' will use CUDA if available, otherwise\n",
      "    CPU.\n",
      "  â†³ Valid values: ['cpu', 'cuda', 'auto']\n",
      "\n",
      "ðŸ“Œ Parameters for get_encoding_model():\n",
      "\n",
      "â€¢ subject (int, required)\n",
      "  â†³ Subject ID from the NSD dataset (1-8)\n",
      "  â†³ Valid values: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "â€¢ nest_dir (str, optional)\n",
      "  â†³ Root directory of the NEST repository (optional if default paths are set)\n",
      "\n",
      "â€¢ selection (dict, required)\n",
      "  â†³ Specifies which outputs to include in the model responses.\n",
      "\n",
      "  â†ª Sub-parameters within 'selection':\n",
      "\n",
      "    â€¢ roi (str)\n",
      "      â†³ Region of Interest (ROI) for voxel prediction. Early visual areas\n",
      "        (V1-V3), category-selective regions (EBA, FFA, etc.), or composite\n",
      "        regions (lateral, ventral).\n",
      "      â†³ Valid values: 'V1', 'V2', 'V3', 'hV4', 'EBA', 'FBA-2', 'OFA', 'FFA-1', 'FFA-2', 'PPA', 'RSC', 'OPA', 'OWFA', 'VWFA-1', 'VWFA-2', 'mfs-words', 'early', 'midventral', 'midlateral', 'midparietal', 'parietal', 'lateral', 'ventral']\n",
      "      â†³ Example: V1\n",
      "\n",
      "ðŸ“Š Performance:\n",
      "\n",
      "  Performance plots at: neural_encoding_simulation_toolkit/encoding_models/modality-fmri/train_dataset-nsd/model-fwrf/encoding_models_accuracy\n",
      "\n",
      "ðŸ“š References:\n",
      "    â€¢ https://doi.org/10.1016/j.neuroimage.2017.06.035\n",
      "    â€¢ https://doi.org/10.1038/s41593-021-00962-x\n",
      "\n",
      "ðŸ“¦ Example Usage:\n",
      "\n",
      "from nest import NEST\n",
      "\n",
      "nest = NEST(\"/path/to/nest_dir\")\n",
      "\n",
      "# Initialize the model\n",
      "model = nest.get_encoding_model(\"fmri-nsd-fwrf\", subject=1, nest_dir='./', selection={'roi': 'V1'})\n",
      "\n",
      "# Generate responses (assuming stimulus is a numpy array)\n",
      "responses = model.generate_response(stimulus)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from nest import NEST\n",
    "nest = NEST(nest_dir=nest_dir)\n",
    "\n",
    "# Choose a model of interest\n",
    "model_id = \"fmri-nsd-fwrf\"  # Example model ID\n",
    "\n",
    "# Get comprehensive model information\n",
    "model_info = nest.describe(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1, ROI V1\n"
     ]
    }
   ],
   "source": [
    "# Load Encoding Model\n",
    "fwrf_model = nest.get_encoding_model(\"fmri-nsd-fwrf\", \n",
    "                                     subject=1, \n",
    "                                     selection={\"roi\": \"V1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding fMRI responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.30it/s, Encoded images=100, Total images=100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.055599  ,  0.1633554 ,  0.0943293 , ...,  0.01447097,\n",
       "         0.15747474,  0.01369774],\n",
       "       [ 0.01819719,  0.07435445,  0.04977052, ..., -0.01315978,\n",
       "         0.10419706,  0.03225161],\n",
       "       [ 0.10229094,  0.21433882,  0.13944887, ...,  0.01901415,\n",
       "         0.14813517,  0.00760403],\n",
       "       ...,\n",
       "       [ 0.04800354, -0.14399454, -0.28155515, ..., -0.02397447,\n",
       "        -0.08016404,  0.00045429],\n",
       "       [ 0.04686468,  0.07014671,  0.00538955, ..., -0.02637868,\n",
       "         0.10473584,  0.02725478],\n",
       "       [ 0.11645083,  0.02392432, -0.04246235, ..., -0.07162039,\n",
       "        -0.0095483 , -0.01467158]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode images\n",
    "nest.encode(fwrf_model, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding fMRI responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.29it/s, Encoded images=100, Total images=100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.055599  ,  0.1633554 ,  0.0943293 , ...,  0.01447097,\n",
       "          0.15747474,  0.01369774],\n",
       "        [ 0.01819719,  0.07435445,  0.04977052, ..., -0.01315978,\n",
       "          0.10419706,  0.03225161],\n",
       "        [ 0.10229094,  0.21433882,  0.13944887, ...,  0.01901415,\n",
       "          0.14813517,  0.00760403],\n",
       "        ...,\n",
       "        [ 0.04800354, -0.14399454, -0.28155515, ..., -0.02397447,\n",
       "         -0.08016404,  0.00045429],\n",
       "        [ 0.04686468,  0.07014671,  0.00538955, ..., -0.02637868,\n",
       "          0.10473584,  0.02725478],\n",
       "        [ 0.11645083,  0.02392432, -0.04246235, ..., -0.07162039,\n",
       "         -0.0095483 , -0.01467158]], dtype=float32),\n",
       " {'fmri': {'ncsnr': array([0.10146447, 0.30089897, 0.2398103 , ..., 0.04201289, 0.22930188,\n",
       "          0.06711598]),\n",
       "   'roi_mask_volume': array([[[False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           ...,\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False]],\n",
       "   \n",
       "          [[False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           ...,\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False]],\n",
       "   \n",
       "          [[False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           ...,\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False]],\n",
       "   \n",
       "          ...,\n",
       "   \n",
       "          [[False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           ...,\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False]],\n",
       "   \n",
       "          [[False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           ...,\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False]],\n",
       "   \n",
       "          [[False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           ...,\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False],\n",
       "           [False, False, False, ..., False, False, False]]]),\n",
       "   'fmri_affine': array([[  1.79999995,   0.        ,   0.        , -72.        ],\n",
       "          [  0.        ,   1.79999995,   0.        , -92.69999695],\n",
       "          [  0.        ,   0.        ,   1.79999995, -73.80000305],\n",
       "          [  0.        ,   0.        ,   0.        ,   1.        ]])},\n",
       "  'encoding_models': {'encoding_accuracy': {'r2': array([6.14280462e-03, 7.66852917e-02, 1.35549856e-02, ...,\n",
       "           7.64837771e-03, 7.42488931e-02, 6.79778458e-06]),\n",
       "    'noise_ceiling': array([0.0299598 , 0.2136019 , 0.14714113, ..., 0.00526736, 0.13624676,\n",
       "           0.01333348]),\n",
       "    'noise_normalized_encoding': array([2.05034892e-01, 3.59010345e-01, 9.21223404e-02, ...,\n",
       "           1.00000000e+00, 5.44958953e-01, 5.09828158e-04])},\n",
       "   'train_val_test_nsd_image_splits': {'train_img_num': array([ 2244, 32051, 19058, ..., 19471, 49507, 11123], dtype=int32),\n",
       "    'val_img_num': array([ 2950,  2990,  3146,  3181,  3386,  3729,  3951,  4058,  4129,\n",
       "            4156,  4325,  4436,  4612,  4768,  4869,  5106,  5285,  5427,\n",
       "            5459,  5502,  5574,  5714,  5878,  5890,  6132,  6222,  6514,\n",
       "            6524,  6558,  6640,  6713,  7336,  7409,  7418,  7480,  7659,\n",
       "            7948,  7954,  8204,  8262,  8274,  8318,  8387,  8415,  8435,\n",
       "            8465,  8631,  8843,  8925,  9230,  9462,  9680,  9722,  9804,\n",
       "            9847,  9865,  9978, 10393, 10507, 10586, 10600, 10610, 10907,\n",
       "           11487, 11566, 11827, 12065, 12075, 12214, 12487, 12495, 12634,\n",
       "           12685, 12798, 12937, 13230, 13653, 13662, 13720, 13846, 14110,\n",
       "           14121, 14165, 14443, 14567, 14644, 14808, 14931, 15003, 15025,\n",
       "           15128, 15492, 15793, 16063, 16421, 16466, 16635, 16655, 16723,\n",
       "           16841, 16865, 16868, 17230, 17238, 17369, 17450, 17463, 17595,\n",
       "           17776, 17794, 17942, 18268, 18483, 18505, 18535, 18690, 18796,\n",
       "           19200, 19292, 19573, 19642, 19672, 19690, 20064, 20223, 20307,\n",
       "           20702, 20777, 21197, 21318, 21508, 21526, 21553, 21703, 21989,\n",
       "           22138, 22143, 22155, 22387, 22505, 22585, 22772, 22782, 22809,\n",
       "           22845, 22873, 22886, 23492, 23871, 23876, 23883, 24264, 24317,\n",
       "           24480, 24638, 24649, 24739, 24740, 24787, 25059, 25205, 25287,\n",
       "           25454, 25570, 25602, 25693, 25711, 25881, 25906, 25959, 26119,\n",
       "           26127, 26243, 26292, 26307, 26372, 26598, 26780, 26971, 27287,\n",
       "           27580, 27830, 27972, 28024, 28286, 28303, 28325, 28341, 28418,\n",
       "           28487, 28689, 28745, 28788, 28898, 28963, 29010, 29021, 29568,\n",
       "           29660, 29663, 29680, 29919, 29980, 30373, 30601, 30674, 30847,\n",
       "           30887, 30922, 30935, 31049, 31064, 31123, 31349, 31446, 31782,\n",
       "           32303, 32643, 32716, 32771, 32857, 33131, 33189, 33289, 33752,\n",
       "           33813, 33906, 34068, 34080, 34089, 34186, 34418, 34603, 34751,\n",
       "           34845, 34874, 34975, 35094, 35129, 35790, 36258, 36569, 36629,\n",
       "           36682, 36759, 37436, 37608, 37736, 37801, 37927, 38060, 38278,\n",
       "           38310, 38483, 39047, 39058, 39095, 39298, 39509, 39547, 39553,\n",
       "           40153, 40235, 40920, 40979, 41056, 41482, 41574, 41623, 41778,\n",
       "           41934, 42126, 42238, 42299, 42642, 42648, 43107, 43156, 43157,\n",
       "           43250, 43288, 43429, 43465, 43500, 43746, 43852, 44097, 44107,\n",
       "           44138, 44184, 44324, 44339, 44369, 44412, 44729, 44736, 45746,\n",
       "           45837, 45982, 45999, 46002, 46036, 46101, 46136, 46160, 46274,\n",
       "           46321, 46480, 46523, 46642, 46835, 46861, 47033, 47099, 47200,\n",
       "           47321, 47598, 47600, 47614, 47656, 48379, 48393, 48617, 48618,\n",
       "           48622, 48832, 48839, 49076, 49152, 49156, 49234, 49622, 49923,\n",
       "           49957, 49969, 50026, 50500, 51051, 51052, 51062, 51171, 51745,\n",
       "           51907, 51928, 51965, 51988, 52216, 52302, 52328, 52375, 52527,\n",
       "           52596, 52598, 52652, 52892, 52990, 53070, 53157, 53270, 53489,\n",
       "           53511, 53858, 54147, 54389, 54643, 54683, 54698, 54743, 54812,\n",
       "           54824, 54959, 55680, 55857, 55933, 55969, 56042, 56066, 56126,\n",
       "           56290, 56418, 56454, 56471, 56670, 56695, 56751, 56782, 57060,\n",
       "           57443, 57478, 57542, 57648, 57681, 57822, 57906, 58096, 58144,\n",
       "           58164, 58187, 58668, 58681, 59039, 59079, 59091, 59419, 59420,\n",
       "           59595, 59631, 60119, 60456, 60519, 60725, 60834, 60867, 60978,\n",
       "           61216, 61513, 61752, 61801, 61972, 62275, 62365, 62479, 62561,\n",
       "           62748, 63081, 63345, 63449, 63746, 63781, 63922, 63944, 64004,\n",
       "           64096, 64483, 64615, 64621, 64687, 64771, 64893, 64979, 65010,\n",
       "           65187, 65232, 65414, 65639, 65654, 65821, 66214, 66342, 66421,\n",
       "           66479, 66643, 67045, 67113, 67168, 67252, 67363, 68023, 68471,\n",
       "           69025, 69030, 69130, 69442, 69502, 69616, 69785, 69831, 69918,\n",
       "           70038, 70335, 70368, 70585, 70589, 70671, 70758, 70764, 70947,\n",
       "           71025, 71186, 71229, 71928, 72080, 72170, 72257, 72510],\n",
       "          dtype=int32),\n",
       "    'test_img_num': array([ 3049,  3077,  3157,  3164,  3171,  3434,  3449,  3489,  3626,\n",
       "            3682,  3687,  3809,  3842,  3847,  3856,  3913,  4051,  4249,\n",
       "            4423,  4667,  4690,  4786,  4835,  4892,  4930,  5034,  5204,\n",
       "            5301,  5338,  5542,  5583,  5602,  6199,  6431,  6444,  6489,\n",
       "            6521,  6801,  7007,  7039,  7120,  7207,  7366,  7654,  7840,\n",
       "            7859,  7944,  8006,  8109,  8225,  8394,  8509,  8646,  8807,\n",
       "            8933,  9027,  9048,  9147,  9434,  9917, 10006, 10046, 10064,\n",
       "           10105, 10107, 10244, 10471, 11159, 11333, 11521, 11617, 11635,\n",
       "           11689, 11725, 11796, 11837, 11844, 11932, 11942, 11952, 12308,\n",
       "           12796, 12922, 13138, 13312, 13314, 13613, 14179, 14528, 14594,\n",
       "           14610, 14793, 14820, 14898, 15364, 15506, 15819, 15939, 16252,\n",
       "           16344, 16616, 16702, 16823, 16917, 17048, 17374, 17485, 17549,\n",
       "           17934, 18434, 18543, 19074, 19181, 19199, 19436, 19933, 20054,\n",
       "           20080, 20206, 20265, 20442, 20633, 20650, 20738, 20820, 21108,\n",
       "           21192, 21194, 21218, 21253, 21279, 21601, 21950, 22263, 22495,\n",
       "           22515, 22523, 22530, 22587, 22794, 22879, 22957, 22967, 22993,\n",
       "           23036, 23241, 23715, 23729, 23993, 24202, 24214, 24517, 24530,\n",
       "           24560, 24640, 24846, 25091, 25111, 25250, 25268, 25284, 25318,\n",
       "           25319, 25371, 25578, 25581, 25588, 25702, 25741, 25746, 26163,\n",
       "           26351, 26435, 26458, 26720, 26895, 26909, 26990, 27242, 27275,\n",
       "           27326, 27435, 27568, 27878, 28068, 28096, 28159, 28189, 28319,\n",
       "           28349, 28524, 28595, 28751, 28755, 29381, 29711, 29837, 29886,\n",
       "           30081, 30395, 30407, 30430, 30563, 30632, 30672, 30856, 30954,\n",
       "           31028, 31233, 31659, 31747, 31801, 31837, 31936, 31964, 32053,\n",
       "           32232, 32307, 32625, 32772, 32891, 32910, 33171, 33245, 33521,\n",
       "           33543, 34126, 34238, 34829, 34850, 35743, 35752, 35798, 35986,\n",
       "           36067, 36476, 36576, 36623, 36731, 36910, 36945, 36974, 36978,\n",
       "           37059, 37221, 37224, 37494, 38022, 38025, 38246, 38297, 38358,\n",
       "           38486, 38494, 38641, 38794, 38817, 38829, 38853, 38978, 39289,\n",
       "           39369, 39402, 39841, 39997, 40423, 40548, 40575, 40721, 40770,\n",
       "           40840, 40846, 40909, 40924, 40935, 41097, 41116, 41137, 41162,\n",
       "           41566, 41653, 41710, 41814, 41927, 42007, 42166, 42171, 42214,\n",
       "           42224, 42473, 42563, 42697, 42781, 42851, 42912, 42946, 42980,\n",
       "           43159, 43164, 43224, 43428, 43445, 43619, 43675, 43689, 43818,\n",
       "           43820, 43950, 43984, 44052, 44144, 44705, 44720, 44844, 44971,\n",
       "           44980, 45129, 45213, 45356, 45595, 45632, 45750, 45843, 45945,\n",
       "           45981, 46150, 46372, 46380, 46460, 46462, 46661, 46806, 47070,\n",
       "           47290, 47293, 47408, 47687, 48157, 48260, 48374, 48422, 48508,\n",
       "           48530, 48679, 48681, 48802, 49466, 49480, 49731, 49944, 49955,\n",
       "           49979, 50114, 50170, 50653, 50734, 50755, 50811, 50882, 51077,\n",
       "           51148, 51185, 51521, 51788, 51843, 51983, 52070, 52394, 52554,\n",
       "           52931, 53052, 53152, 53155, 53370, 53570, 53727, 53728, 53773,\n",
       "           53881, 53891, 54078, 54257, 54361, 54913, 55005, 55107, 55163,\n",
       "           55405, 55408, 55526, 55649, 55669, 55678, 55968, 56154, 56269,\n",
       "           56314, 56681, 56723, 56784, 56850, 56867, 56911, 56948, 57046,\n",
       "           57435, 57553, 57650, 57838, 58404, 59023, 59046, 59194, 59284,\n",
       "           59585, 59699, 59817, 59994, 60094, 60186, 60251, 60305, 60505,\n",
       "           60553, 60845, 61122, 61133, 61177, 61510, 61677, 61738, 61748,\n",
       "           61797, 61809, 61873, 62006, 62015, 62209, 62228, 62302, 62544,\n",
       "           62683, 62960, 63182, 63825, 63931, 64295, 64498, 64867, 64880,\n",
       "           65069, 65148, 65253, 65267, 65376, 65445, 65686, 65769, 65799,\n",
       "           65872, 65920, 65943, 66004, 66216, 66278, 66330, 66464, 66489,\n",
       "           66580, 66773, 66836, 66946, 66976, 67204, 67237, 67295, 67742,\n",
       "           67802, 67829, 68168, 68278, 68311, 68339, 68418, 68741, 68814,\n",
       "           68842, 68858, 68897, 69007, 69214, 69240, 69614, 69813, 69839,\n",
       "           69854, 70075, 70095, 70193, 70232, 70360, 70427, 70505, 71232,\n",
       "           71241, 71410, 71450, 71753, 71894, 72015, 72209, 72312, 72605,\n",
       "           72719, 72948], dtype=int32)}}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode images and return metadata\n",
    "nest.encode(fwrf_model, images, return_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fmri': {'ncsnr': array([0.10146447, 0.30089897, 0.2398103 , ..., 0.04201289, 0.22930188,\n",
       "         0.06711598]),\n",
       "  'roi_mask_volume': array([[[False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False]],\n",
       "  \n",
       "         [[False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False]],\n",
       "  \n",
       "         [[False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False]],\n",
       "  \n",
       "         [[False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False]],\n",
       "  \n",
       "         [[False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False],\n",
       "          [False, False, False, ..., False, False, False]]]),\n",
       "  'fmri_affine': array([[  1.79999995,   0.        ,   0.        , -72.        ],\n",
       "         [  0.        ,   1.79999995,   0.        , -92.69999695],\n",
       "         [  0.        ,   0.        ,   1.79999995, -73.80000305],\n",
       "         [  0.        ,   0.        ,   0.        ,   1.        ]])},\n",
       " 'encoding_models': {'encoding_accuracy': {'r2': array([6.14280462e-03, 7.66852917e-02, 1.35549856e-02, ...,\n",
       "          7.64837771e-03, 7.42488931e-02, 6.79778458e-06]),\n",
       "   'noise_ceiling': array([0.0299598 , 0.2136019 , 0.14714113, ..., 0.00526736, 0.13624676,\n",
       "          0.01333348]),\n",
       "   'noise_normalized_encoding': array([2.05034892e-01, 3.59010345e-01, 9.21223404e-02, ...,\n",
       "          1.00000000e+00, 5.44958953e-01, 5.09828158e-04])},\n",
       "  'train_val_test_nsd_image_splits': {'train_img_num': array([ 2244, 32051, 19058, ..., 19471, 49507, 11123], dtype=int32),\n",
       "   'val_img_num': array([ 2950,  2990,  3146,  3181,  3386,  3729,  3951,  4058,  4129,\n",
       "           4156,  4325,  4436,  4612,  4768,  4869,  5106,  5285,  5427,\n",
       "           5459,  5502,  5574,  5714,  5878,  5890,  6132,  6222,  6514,\n",
       "           6524,  6558,  6640,  6713,  7336,  7409,  7418,  7480,  7659,\n",
       "           7948,  7954,  8204,  8262,  8274,  8318,  8387,  8415,  8435,\n",
       "           8465,  8631,  8843,  8925,  9230,  9462,  9680,  9722,  9804,\n",
       "           9847,  9865,  9978, 10393, 10507, 10586, 10600, 10610, 10907,\n",
       "          11487, 11566, 11827, 12065, 12075, 12214, 12487, 12495, 12634,\n",
       "          12685, 12798, 12937, 13230, 13653, 13662, 13720, 13846, 14110,\n",
       "          14121, 14165, 14443, 14567, 14644, 14808, 14931, 15003, 15025,\n",
       "          15128, 15492, 15793, 16063, 16421, 16466, 16635, 16655, 16723,\n",
       "          16841, 16865, 16868, 17230, 17238, 17369, 17450, 17463, 17595,\n",
       "          17776, 17794, 17942, 18268, 18483, 18505, 18535, 18690, 18796,\n",
       "          19200, 19292, 19573, 19642, 19672, 19690, 20064, 20223, 20307,\n",
       "          20702, 20777, 21197, 21318, 21508, 21526, 21553, 21703, 21989,\n",
       "          22138, 22143, 22155, 22387, 22505, 22585, 22772, 22782, 22809,\n",
       "          22845, 22873, 22886, 23492, 23871, 23876, 23883, 24264, 24317,\n",
       "          24480, 24638, 24649, 24739, 24740, 24787, 25059, 25205, 25287,\n",
       "          25454, 25570, 25602, 25693, 25711, 25881, 25906, 25959, 26119,\n",
       "          26127, 26243, 26292, 26307, 26372, 26598, 26780, 26971, 27287,\n",
       "          27580, 27830, 27972, 28024, 28286, 28303, 28325, 28341, 28418,\n",
       "          28487, 28689, 28745, 28788, 28898, 28963, 29010, 29021, 29568,\n",
       "          29660, 29663, 29680, 29919, 29980, 30373, 30601, 30674, 30847,\n",
       "          30887, 30922, 30935, 31049, 31064, 31123, 31349, 31446, 31782,\n",
       "          32303, 32643, 32716, 32771, 32857, 33131, 33189, 33289, 33752,\n",
       "          33813, 33906, 34068, 34080, 34089, 34186, 34418, 34603, 34751,\n",
       "          34845, 34874, 34975, 35094, 35129, 35790, 36258, 36569, 36629,\n",
       "          36682, 36759, 37436, 37608, 37736, 37801, 37927, 38060, 38278,\n",
       "          38310, 38483, 39047, 39058, 39095, 39298, 39509, 39547, 39553,\n",
       "          40153, 40235, 40920, 40979, 41056, 41482, 41574, 41623, 41778,\n",
       "          41934, 42126, 42238, 42299, 42642, 42648, 43107, 43156, 43157,\n",
       "          43250, 43288, 43429, 43465, 43500, 43746, 43852, 44097, 44107,\n",
       "          44138, 44184, 44324, 44339, 44369, 44412, 44729, 44736, 45746,\n",
       "          45837, 45982, 45999, 46002, 46036, 46101, 46136, 46160, 46274,\n",
       "          46321, 46480, 46523, 46642, 46835, 46861, 47033, 47099, 47200,\n",
       "          47321, 47598, 47600, 47614, 47656, 48379, 48393, 48617, 48618,\n",
       "          48622, 48832, 48839, 49076, 49152, 49156, 49234, 49622, 49923,\n",
       "          49957, 49969, 50026, 50500, 51051, 51052, 51062, 51171, 51745,\n",
       "          51907, 51928, 51965, 51988, 52216, 52302, 52328, 52375, 52527,\n",
       "          52596, 52598, 52652, 52892, 52990, 53070, 53157, 53270, 53489,\n",
       "          53511, 53858, 54147, 54389, 54643, 54683, 54698, 54743, 54812,\n",
       "          54824, 54959, 55680, 55857, 55933, 55969, 56042, 56066, 56126,\n",
       "          56290, 56418, 56454, 56471, 56670, 56695, 56751, 56782, 57060,\n",
       "          57443, 57478, 57542, 57648, 57681, 57822, 57906, 58096, 58144,\n",
       "          58164, 58187, 58668, 58681, 59039, 59079, 59091, 59419, 59420,\n",
       "          59595, 59631, 60119, 60456, 60519, 60725, 60834, 60867, 60978,\n",
       "          61216, 61513, 61752, 61801, 61972, 62275, 62365, 62479, 62561,\n",
       "          62748, 63081, 63345, 63449, 63746, 63781, 63922, 63944, 64004,\n",
       "          64096, 64483, 64615, 64621, 64687, 64771, 64893, 64979, 65010,\n",
       "          65187, 65232, 65414, 65639, 65654, 65821, 66214, 66342, 66421,\n",
       "          66479, 66643, 67045, 67113, 67168, 67252, 67363, 68023, 68471,\n",
       "          69025, 69030, 69130, 69442, 69502, 69616, 69785, 69831, 69918,\n",
       "          70038, 70335, 70368, 70585, 70589, 70671, 70758, 70764, 70947,\n",
       "          71025, 71186, 71229, 71928, 72080, 72170, 72257, 72510],\n",
       "         dtype=int32),\n",
       "   'test_img_num': array([ 3049,  3077,  3157,  3164,  3171,  3434,  3449,  3489,  3626,\n",
       "           3682,  3687,  3809,  3842,  3847,  3856,  3913,  4051,  4249,\n",
       "           4423,  4667,  4690,  4786,  4835,  4892,  4930,  5034,  5204,\n",
       "           5301,  5338,  5542,  5583,  5602,  6199,  6431,  6444,  6489,\n",
       "           6521,  6801,  7007,  7039,  7120,  7207,  7366,  7654,  7840,\n",
       "           7859,  7944,  8006,  8109,  8225,  8394,  8509,  8646,  8807,\n",
       "           8933,  9027,  9048,  9147,  9434,  9917, 10006, 10046, 10064,\n",
       "          10105, 10107, 10244, 10471, 11159, 11333, 11521, 11617, 11635,\n",
       "          11689, 11725, 11796, 11837, 11844, 11932, 11942, 11952, 12308,\n",
       "          12796, 12922, 13138, 13312, 13314, 13613, 14179, 14528, 14594,\n",
       "          14610, 14793, 14820, 14898, 15364, 15506, 15819, 15939, 16252,\n",
       "          16344, 16616, 16702, 16823, 16917, 17048, 17374, 17485, 17549,\n",
       "          17934, 18434, 18543, 19074, 19181, 19199, 19436, 19933, 20054,\n",
       "          20080, 20206, 20265, 20442, 20633, 20650, 20738, 20820, 21108,\n",
       "          21192, 21194, 21218, 21253, 21279, 21601, 21950, 22263, 22495,\n",
       "          22515, 22523, 22530, 22587, 22794, 22879, 22957, 22967, 22993,\n",
       "          23036, 23241, 23715, 23729, 23993, 24202, 24214, 24517, 24530,\n",
       "          24560, 24640, 24846, 25091, 25111, 25250, 25268, 25284, 25318,\n",
       "          25319, 25371, 25578, 25581, 25588, 25702, 25741, 25746, 26163,\n",
       "          26351, 26435, 26458, 26720, 26895, 26909, 26990, 27242, 27275,\n",
       "          27326, 27435, 27568, 27878, 28068, 28096, 28159, 28189, 28319,\n",
       "          28349, 28524, 28595, 28751, 28755, 29381, 29711, 29837, 29886,\n",
       "          30081, 30395, 30407, 30430, 30563, 30632, 30672, 30856, 30954,\n",
       "          31028, 31233, 31659, 31747, 31801, 31837, 31936, 31964, 32053,\n",
       "          32232, 32307, 32625, 32772, 32891, 32910, 33171, 33245, 33521,\n",
       "          33543, 34126, 34238, 34829, 34850, 35743, 35752, 35798, 35986,\n",
       "          36067, 36476, 36576, 36623, 36731, 36910, 36945, 36974, 36978,\n",
       "          37059, 37221, 37224, 37494, 38022, 38025, 38246, 38297, 38358,\n",
       "          38486, 38494, 38641, 38794, 38817, 38829, 38853, 38978, 39289,\n",
       "          39369, 39402, 39841, 39997, 40423, 40548, 40575, 40721, 40770,\n",
       "          40840, 40846, 40909, 40924, 40935, 41097, 41116, 41137, 41162,\n",
       "          41566, 41653, 41710, 41814, 41927, 42007, 42166, 42171, 42214,\n",
       "          42224, 42473, 42563, 42697, 42781, 42851, 42912, 42946, 42980,\n",
       "          43159, 43164, 43224, 43428, 43445, 43619, 43675, 43689, 43818,\n",
       "          43820, 43950, 43984, 44052, 44144, 44705, 44720, 44844, 44971,\n",
       "          44980, 45129, 45213, 45356, 45595, 45632, 45750, 45843, 45945,\n",
       "          45981, 46150, 46372, 46380, 46460, 46462, 46661, 46806, 47070,\n",
       "          47290, 47293, 47408, 47687, 48157, 48260, 48374, 48422, 48508,\n",
       "          48530, 48679, 48681, 48802, 49466, 49480, 49731, 49944, 49955,\n",
       "          49979, 50114, 50170, 50653, 50734, 50755, 50811, 50882, 51077,\n",
       "          51148, 51185, 51521, 51788, 51843, 51983, 52070, 52394, 52554,\n",
       "          52931, 53052, 53152, 53155, 53370, 53570, 53727, 53728, 53773,\n",
       "          53881, 53891, 54078, 54257, 54361, 54913, 55005, 55107, 55163,\n",
       "          55405, 55408, 55526, 55649, 55669, 55678, 55968, 56154, 56269,\n",
       "          56314, 56681, 56723, 56784, 56850, 56867, 56911, 56948, 57046,\n",
       "          57435, 57553, 57650, 57838, 58404, 59023, 59046, 59194, 59284,\n",
       "          59585, 59699, 59817, 59994, 60094, 60186, 60251, 60305, 60505,\n",
       "          60553, 60845, 61122, 61133, 61177, 61510, 61677, 61738, 61748,\n",
       "          61797, 61809, 61873, 62006, 62015, 62209, 62228, 62302, 62544,\n",
       "          62683, 62960, 63182, 63825, 63931, 64295, 64498, 64867, 64880,\n",
       "          65069, 65148, 65253, 65267, 65376, 65445, 65686, 65769, 65799,\n",
       "          65872, 65920, 65943, 66004, 66216, 66278, 66330, 66464, 66489,\n",
       "          66580, 66773, 66836, 66946, 66976, 67204, 67237, 67295, 67742,\n",
       "          67802, 67829, 68168, 68278, 68311, 68339, 68418, 68741, 68814,\n",
       "          68842, 68858, 68897, 69007, 69214, 69240, 69614, 69813, 69839,\n",
       "          69854, 70075, 70095, 70193, 70232, 70360, 70427, 70505, 71232,\n",
       "          71241, 71410, 71450, 71753, 71894, 72015, 72209, 72312, 72605,\n",
       "          72719, 72948], dtype=int32)}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just return metadata of model\n",
    "fwrf_model.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing EEG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1\n"
     ]
    }
   ],
   "source": [
    "from nest import NEST\n",
    "nest = NEST(nest_dir=nest_dir)\n",
    "\n",
    "\n",
    "eeg_model = nest.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", \n",
    "                                    subject=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest.encode(eeg_model, images,return_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Selection Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding EEG responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.79s/it, Encoded images=100, Total images=100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 63, 140)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feeg_model = nest.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", \n",
    "                                    subject=1,\n",
    "                                    device=\"auto\",\n",
    "                                    selection={\"channels\": ['F7']})\n",
    "\n",
    "eeg_silico = nest.encode(eeg_model, images)\n",
    "\n",
    "print(eeg_silico.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding EEG responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.77s/it, Encoded images=100, Total images=100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 1, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, \n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "          0, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0]\n",
    "\n",
    "\n",
    "eeg_model = nest.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", \n",
    "                                    subject=1,\n",
    "                                    device=\"auto\",\n",
    "                                    selection={\"channels\": ['F7'],\n",
    "                                               \"timepoints\": vector})\n",
    "\n",
    "eeg_silico = nest.encode(eeg_model, images)\n",
    "\n",
    "print(eeg_silico.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can describe a model in two ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§  Model: eeg-things_eeg_2-vit_b_32\n",
      "================================================================================\n",
      "\n",
      "Modality: eeg\n",
      "Training dataset: things_eeg_2\n",
      "Model architecture: vision transformer (ViT-B/32)\n",
      "Creator: Alessandro Gifford\n",
      "\n",
      "ðŸ“‹ Description:\n",
      "This model generates in silico EEG responses to visual stimuli using a vision\n",
      "transformer model. It was trained on the THINGS-EEG-2 dataset, which contains\n",
      "EEG recordings from subjects viewing images of everyday objects. The model\n",
      "extracts visual features using a pre-trained ViT-B/32 transformer, applies\n",
      "dimensionality reduction, and then predicts EEG responses across all channels\n",
      "and time points.  The model takes as input a batch of RGB images in the shape\n",
      "[batch_size, 3, height, width], with pixel values ranging from 0 to 255 and\n",
      "square dimensions (e.g., 224Ã—224).\n",
      "\n",
      "ðŸ“¥ Input:\n",
      "  Type: numpy.ndarray\n",
      "  Shape: ['batch_size', 3, 'height', 'width']\n",
      "  Description: The input should be a batch of RGB images.\n",
      "  Constraints:\n",
      "    â€¢ Image values should be integers in range [0, 255]\n",
      "    â€¢ Image dimensions (height, width) should be equal (square)\n",
      "    â€¢ Minimum recommended image size: 224x224 pixels\n",
      "\n",
      "ðŸ“¤ Output:\n",
      "  Type: numpy.ndarray\n",
      "  Shape: ['batch_size', 'n_repetitions', 'n_channels', 'n_timepoints']\n",
      "  Description: The output is a 4D array containing predicted EEG responses.\n",
      "  Dimensions:\n",
      "    â€¢ batch_size: Number of stimuli in the batch\n",
      "    â€¢ n_repetitions: Number of simulated repetitions of the same stimulus (typically 4)\n",
      "    â€¢ n_channels: Number of EEG channels (typically 63)\n",
      "    â€¢ n_timepoints: Number of time points in the EEG epoch (typically 140)\n",
      "\n",
      "ðŸ“Œ Parameters for encode():\n",
      "\n",
      "â€¢ stimulus (numpy.ndarray, required)\n",
      "  â†³ A batch of RGB images to be encoded. Images should be in integer format with\n",
      "    values in the range [0, 255], and square dimensions (e.g. 224x224).\n",
      "\n",
      "â€¢ device (str, optional, default='auto')\n",
      "  â†³ Device to run the model on. 'auto' will use CUDA if available, otherwise\n",
      "    CPU.\n",
      "  â†³ Valid values: ['cpu', 'cuda', 'auto']\n",
      "\n",
      "â€¢ show_progress (bool, optional, default=True)\n",
      "  â†³ Whether to show a progress bar during encoding (for large batches)\n",
      "\n",
      "ðŸ“Œ Parameters for get_encoding_model():\n",
      "\n",
      "â€¢ subject (int, required)\n",
      "  â†³ Subject ID from the THINGS-EEG-2 dataset (1-10)\n",
      "  â†³ Valid values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "â€¢ nest_dir (str, optional)\n",
      "  â†³ Root directory of the NEST repository (optional if default paths are set)\n",
      "\n",
      "â€¢ selection (dict, optional)\n",
      "  â†³ Specifies which outputs to include in the model responses. Can include\n",
      "    specific channels and/or timepoints.\n",
      "\n",
      "  â†ª Sub-parameters within 'selection':\n",
      "\n",
      "    â€¢ channels (list[str])\n",
      "      â†³ List of EEG channel names to include in the output\n",
      "      â†³ Valid values: 'Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz', 'F1', 'F5', 'FT7', 'FC3', 'FCz', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'P6', 'P2', 'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4', 'FT8', 'F6', 'F2', 'AF4', 'AF8']\n",
      "      â†³ Example: ['Oz', 'Cz', 'Fp1']\n",
      "\n",
      "    â€¢ timepoints (binary_vector)\n",
      "      â†³ Binary one-hot encoded vector indicating which timepoints to include.\n",
      "        Must have exactly the same length as the number of available timepoints\n",
      "        (140). Each position set to 1 indicates that timepoint should be\n",
      "        included.\n",
      "      â†³ Example: [0, 0, 0, 1, 1, 1, 0, 0]\n",
      "\n",
      "ðŸ“Š Performance:\n",
      "\n",
      "  Performance plots at: neural_encoding_simulation_toolkit/encoding_models/modality-eeg/train_dataset-things_eeg_2/model-vit_b_32/encoding_models_accuracy\n",
      "\n",
      "ðŸ“š References:\n",
      "    â€¢ https://doi.org/10.1038/s41593-021-00962-x\n",
      "\n",
      "ðŸ“¦ Example Usage:\n",
      "\n",
      "from nest import NEST\n",
      "\n",
      "nest = NEST(\"/path/to/nest_dir\")\n",
      "\n",
      "# Initialize the model\n",
      "model = nest.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", subject=1, nest_dir='./', selection={'channels': ['Oz', 'Cz', 'Fp1'], 'timepoints': [0, 0, 0, 1, 1, 1, 0, 0]})\n",
      "\n",
      "# Generate responses (assuming stimulus is a numpy array)\n",
      "responses = model.generate_response(stimulus)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "description = nest.describe(\"eeg-things_eeg_2-vit_b_32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§  Model: eeg-things_eeg_2-vit_b_32\n",
      "================================================================================\n",
      "\n",
      "Modality: eeg\n",
      "Training dataset: things_eeg_2\n",
      "Model architecture: vision transformer (ViT-B/32)\n",
      "Creator: Alessandro Gifford\n",
      "\n",
      "ðŸ“‹ Description:\n",
      "This model generates in silico EEG responses to visual stimuli using a vision\n",
      "transformer model. It was trained on the THINGS-EEG-2 dataset, which contains\n",
      "EEG recordings from subjects viewing images of everyday objects. The model\n",
      "extracts visual features using a pre-trained ViT-B/32 transformer, applies\n",
      "dimensionality reduction, and then predicts EEG responses across all channels\n",
      "and time points.  The model takes as input a batch of RGB images in the shape\n",
      "[batch_size, 3, height, width], with pixel values ranging from 0 to 255 and\n",
      "square dimensions (e.g., 224Ã—224).\n",
      "\n",
      "ðŸ“¥ Input:\n",
      "  Type: numpy.ndarray\n",
      "  Shape: ['batch_size', 3, 'height', 'width']\n",
      "  Description: The input should be a batch of RGB images.\n",
      "  Constraints:\n",
      "    â€¢ Image values should be integers in range [0, 255]\n",
      "    â€¢ Image dimensions (height, width) should be equal (square)\n",
      "    â€¢ Minimum recommended image size: 224x224 pixels\n",
      "\n",
      "ðŸ“¤ Output:\n",
      "  Type: numpy.ndarray\n",
      "  Shape: ['batch_size', 'n_repetitions', 'n_channels', 'n_timepoints']\n",
      "  Description: The output is a 4D array containing predicted EEG responses.\n",
      "  Dimensions:\n",
      "    â€¢ batch_size: Number of stimuli in the batch\n",
      "    â€¢ n_repetitions: Number of simulated repetitions of the same stimulus (typically 4)\n",
      "    â€¢ n_channels: Number of EEG channels (typically 63)\n",
      "    â€¢ n_timepoints: Number of time points in the EEG epoch (typically 140)\n",
      "\n",
      "ðŸ“Œ Parameters for encode():\n",
      "\n",
      "â€¢ stimulus (numpy.ndarray, required)\n",
      "  â†³ A batch of RGB images to be encoded. Images should be in integer format with\n",
      "    values in the range [0, 255], and square dimensions (e.g. 224x224).\n",
      "\n",
      "â€¢ device (str, optional, default='auto')\n",
      "  â†³ Device to run the model on. 'auto' will use CUDA if available, otherwise\n",
      "    CPU.\n",
      "  â†³ Valid values: ['cpu', 'cuda', 'auto']\n",
      "\n",
      "â€¢ show_progress (bool, optional, default=True)\n",
      "  â†³ Whether to show a progress bar during encoding (for large batches)\n",
      "\n",
      "ðŸ“Œ Parameters for get_encoding_model():\n",
      "\n",
      "â€¢ subject (int, required)\n",
      "  â†³ Subject ID from the THINGS-EEG-2 dataset (1-10)\n",
      "  â†³ Valid values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "â€¢ nest_dir (str, optional)\n",
      "  â†³ Root directory of the NEST repository (optional if default paths are set)\n",
      "\n",
      "â€¢ selection (dict, optional)\n",
      "  â†³ Specifies which outputs to include in the model responses. Can include\n",
      "    specific channels and/or timepoints.\n",
      "\n",
      "  â†ª Sub-parameters within 'selection':\n",
      "\n",
      "    â€¢ channels (list[str])\n",
      "      â†³ List of EEG channel names to include in the output\n",
      "      â†³ Valid values: 'Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz', 'F1', 'F5', 'FT7', 'FC3', 'FCz', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'P6', 'P2', 'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4', 'FT8', 'F6', 'F2', 'AF4', 'AF8']\n",
      "      â†³ Example: ['Oz', 'Cz', 'Fp1']\n",
      "\n",
      "    â€¢ timepoints (binary_vector)\n",
      "      â†³ Binary one-hot encoded vector indicating which timepoints to include.\n",
      "        Must have exactly the same length as the number of available timepoints\n",
      "        (140). Each position set to 1 indicates that timepoint should be\n",
      "        included.\n",
      "      â†³ Example: [0, 0, 0, 1, 1, 1, 0, 0]\n",
      "\n",
      "ðŸ“Š Performance:\n",
      "\n",
      "  Performance plots at: neural_encoding_simulation_toolkit/encoding_models/modality-eeg/train_dataset-things_eeg_2/model-vit_b_32/encoding_models_accuracy\n",
      "\n",
      "ðŸ“š References:\n",
      "    â€¢ https://doi.org/10.1038/s41593-021-00962-x\n",
      "\n",
      "ðŸ“¦ Example Usage:\n",
      "\n",
      "from nest import NEST\n",
      "\n",
      "nest = NEST(\"/path/to/nest_dir\")\n",
      "\n",
      "# Initialize the model\n",
      "model = nest.get_encoding_model(\"eeg-things_eeg_2-vit_b_32\", subject=1, nest_dir='./', selection={'channels': ['Oz', 'Cz', 'Fp1'], 'timepoints': [0, 0, 0, 1, 1, 1, 0, 0]})\n",
      "\n",
      "# Generate responses (assuming stimulus is a numpy array)\n",
      "responses = model.generate_response(stimulus)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "description = eeg_model.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
