{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"\")))\n",
    "\n",
    "from new_nest.models.fmri.nsd_fwrf import FMRIEncodingModel\n",
    "\n",
    "mymodel = FMRIEncodingModel(subject=1, roi=\"V1\", nest_dir=\"./\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1, ROI V1\n"
     ]
    }
   ],
   "source": [
    "mymodel.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:\n",
      "1: 000000000139.jpg\n",
      "2: 000000000285.jpg\n",
      "3: 000000000632.jpg\n",
      "4: 000000000724.jpg\n",
      "5: 000000000776.jpg\n",
      "6: 000000000785.jpg\n",
      "7: 000000000802.jpg\n",
      "8: 000000000872.jpg\n",
      "9: 000000000885.jpg\n",
      "10: 000000001000.jpg\n",
      "11: 000000001268.jpg\n",
      "12: 000000001296.jpg\n",
      "13: 000000001353.jpg\n",
      "14: 000000001425.jpg\n",
      "15: 000000001490.jpg\n",
      "16: 000000001503.jpg\n",
      "17: 000000001532.jpg\n",
      "18: 000000001584.jpg\n",
      "19: 000000001675.jpg\n",
      "20: 000000001761.jpg\n",
      "21: 000000001818.jpg\n",
      "22: 000000001993.jpg\n",
      "23: 000000002006.jpg\n",
      "24: 000000002149.jpg\n",
      "25: 000000002153.jpg\n",
      "26: 000000002157.jpg\n",
      "27: 000000002261.jpg\n",
      "28: 000000002299.jpg\n",
      "29: 000000002431.jpg\n",
      "30: 000000002473.jpg\n",
      "31: 000000002532.jpg\n",
      "32: 000000002587.jpg\n",
      "33: 000000002592.jpg\n",
      "34: 000000002685.jpg\n",
      "35: 000000002923.jpg\n",
      "36: 000000003156.jpg\n",
      "37: 000000003255.jpg\n",
      "38: 000000003501.jpg\n",
      "39: 000000003553.jpg\n",
      "40: 000000003661.jpg\n",
      "41: 000000003845.jpg\n",
      "42: 000000003934.jpg\n",
      "43: 000000004134.jpg\n",
      "44: 000000004395.jpg\n",
      "45: 000000004495.jpg\n",
      "46: 000000004765.jpg\n",
      "47: 000000004795.jpg\n",
      "48: 000000005001.jpg\n",
      "49: 000000005037.jpg\n",
      "50: 000000005060.jpg\n",
      "51: 000000005193.jpg\n",
      "52: 000000005477.jpg\n",
      "53: 000000005503.jpg\n",
      "54: 000000005529.jpg\n",
      "55: 000000005586.jpg\n",
      "56: 000000005600.jpg\n",
      "57: 000000005992.jpg\n",
      "58: 000000006012.jpg\n",
      "59: 000000006040.jpg\n",
      "60: 000000006213.jpg\n",
      "61: 000000006460.jpg\n",
      "62: 000000006471.jpg\n",
      "63: 000000006614.jpg\n",
      "64: 000000006723.jpg\n",
      "65: 000000006763.jpg\n",
      "66: 000000006771.jpg\n",
      "67: 000000006818.jpg\n",
      "68: 000000006894.jpg\n",
      "69: 000000006954.jpg\n",
      "70: 000000007088.jpg\n",
      "71: 000000007108.jpg\n",
      "72: 000000007278.jpg\n",
      "73: 000000007281.jpg\n",
      "74: 000000007386.jpg\n",
      "75: 000000007511.jpg\n",
      "76: 000000007574.jpg\n",
      "77: 000000007784.jpg\n",
      "78: 000000007795.jpg\n",
      "79: 000000007816.jpg\n",
      "80: 000000007818.jpg\n",
      "81: 000000007888.jpg\n",
      "82: 000000007977.jpg\n",
      "83: 000000007991.jpg\n",
      "84: 000000008021.jpg\n",
      "85: 000000008211.jpg\n",
      "86: 000000008277.jpg\n",
      "87: 000000008532.jpg\n",
      "88: 000000008629.jpg\n",
      "89: 000000008690.jpg\n",
      "90: 000000008762.jpg\n",
      "91: 000000008844.jpg\n",
      "92: 000000008899.jpg\n",
      "93: 000000009378.jpg\n",
      "94: 000000009400.jpg\n",
      "95: 000000009448.jpg\n",
      "96: 000000009483.jpg\n",
      "97: 000000009590.jpg\n",
      "98: 000000009769.jpg\n",
      "99: 000000009772.jpg\n",
      "100: 000000009891.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 299.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Images shape:\n",
      "(100, 3, 227, 227)\n",
      "(Batch size Ã— 3 RGB Channels x Width x Height)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import transforms as trn\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "images_dir = os.path.join('tutorial_images')\n",
    "images_list = os.listdir(images_dir)\n",
    "images_list.sort()\n",
    "print('Images:')\n",
    "for i, img in enumerate(images_list):\n",
    "    print(str(i+1) + ': ' + img)\n",
    "    \n",
    "    \n",
    "images = []\n",
    "for img in tqdm(images_list):\n",
    "    img_dir = os.path.join(images_dir, img)\n",
    "    img = Image.open(img_dir).convert('RGB')\n",
    "    # Center crop the images to square format, and resize them\n",
    "    transform = trn.Compose([\n",
    "        trn.CenterCrop(min(img.size)),\n",
    "        trn.Resize((227,227))\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = np.asarray(img)\n",
    "    img = img.transpose(2,0,1)\n",
    "    images.append(img)\n",
    "images = np.asarray(images)\n",
    "\n",
    "# Print the images dimensions\n",
    "print('\\n\\nImages shape:')\n",
    "print(images.shape)\n",
    "print('(Batch size Ã— 3 RGB Channels x Width x Height)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding fMRI responses:   0%|          | 0/1 [00:00<?, ?it/s, Encoded images=0, Total images=100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding fMRI responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.02s/it, Encoded images=100, Total images=100]\n"
     ]
    }
   ],
   "source": [
    "insilico = mymodel.generate_response(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.055599  ,  0.1633554 ,  0.0943293 , ...,  0.01447097,\n",
       "         0.15747474,  0.01369774],\n",
       "       [ 0.01819719,  0.07435445,  0.04977052, ..., -0.01315978,\n",
       "         0.10419706,  0.03225161],\n",
       "       [ 0.10229094,  0.21433882,  0.13944887, ...,  0.01901415,\n",
       "         0.14813517,  0.00760403],\n",
       "       ...,\n",
       "       [ 0.04800354, -0.14399454, -0.28155515, ..., -0.02397447,\n",
       "        -0.08016404,  0.00045429],\n",
       "       [ 0.04686468,  0.07014671,  0.00538955, ..., -0.02637868,\n",
       "         0.10473584,  0.02725478],\n",
       "       [ 0.11645083,  0.02392432, -0.04246235, ..., -0.07162039,\n",
       "        -0.0095483 , -0.01467158]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insilico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: Torch_LayerwiseFWRF(\n",
       "    (sm): Softmax(dim=1)\n",
       "  )}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.subject_fwrfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1, ROI V1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Torch_LayerwiseFWRF(\n",
       "  (sm): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.subject_fwrfs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nest.utils import get_model_fmri_nsd_fwrf\n",
    "\n",
    "encoding_model = get_model_fmri_nsd_fwrf(\"./\", 1, \"V1\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shared_model': Encoder(\n",
       "   (pre): PreFilter(\n",
       "     (conv1): Sequential(\n",
       "       (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "       (1): ReLU(inplace=True)\n",
       "       (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "     (conv2): Sequential(\n",
       "       (0): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "       (1): ReLU(inplace=True)\n",
       "     )\n",
       "   )\n",
       "   (enc): EncStage(\n",
       "     (conv3): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "     (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "     (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "     (conv4a): TrunkBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (conv5a): TrunkBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (conv6a): TrunkBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (conv4b): TrunkBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (conv5b): TrunkBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (conv6b): TrunkBlock(\n",
       "       (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.25, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'subject_fwrfs': {1: Torch_LayerwiseFWRF(\n",
       "    (sm): Softmax(dim=1)\n",
       "  )},\n",
       " 'nnv': {1: 1350},\n",
       " 'resize_px': 227}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test finished fwRF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"\")))\n",
    "\n",
    "from new_nest.models.fmri.nsd_fwrf import FMRIEncodingModel\n",
    "\n",
    "mymodel = FMRIEncodingModel(subject=1, roi=\"V1\", nest_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'FMRIEncodingModel' has no attribute 'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmymodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Repositories/NEST/new_nest/interfaces/base_model.py:81\u001b[0m, in \u001b[0;36mBaseModelInterface.describe\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mReturn a detailed, human-readable description of the model,\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mincluding metadata, supported parameters, and usage example.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Dict containing description fields, and prints a nicely formatted summary.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_id()\n\u001b[0;32m---> 81\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_parameters()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Build example parameter string for usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repositories/NEST/new_nest/models/fmri/nsd_fwrf.py:276\u001b[0m, in \u001b[0;36mFMRIEncodingModel.get_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    266\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    Return metadata about the model.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m        Dict containing model metadata\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodality\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmri\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnsd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfwrf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubject\u001b[49m,\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi,\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m227\u001b[39m, \u001b[38;5;241m227\u001b[39m],  \u001b[38;5;66;03m# [channels, height, width]\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1000\u001b[39m]  \u001b[38;5;66;03m# [voxels] - would be actual number\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     }\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'FMRIEncodingModel' has no attribute 'subject'"
     ]
    }
   ],
   "source": [
    "mymodel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NEST Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_nest import NEST\n",
    "nest = NEST(nest_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: {'fmri_nsd_fwrf': ['1.0.0']}\n",
      "Available modalities: ['fmri']\n"
     ]
    }
   ],
   "source": [
    "# List all available models and their versions\n",
    "available_models = nest.list_models()\n",
    "print(f\"Available models: {available_models}\")\n",
    "\n",
    "# See what modalities are available\n",
    "modalities = nest.which_modalities()\n",
    "print(f\"Available modalities: {modalities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ§  Model: fmri_nsd_fwrf\n",
      "------------------------------------------------------------\n",
      "Modality: fmri\n",
      "Dataset: NSD\n",
      "Features: x\n",
      "Repeats: x\n",
      "Subject level: x\n",
      "\n",
      "ðŸ“Œ Supported Parameters:\n",
      "\n",
      "â€¢ subject (int, required)\n",
      "  â†³ Subject ID from the NSD dataset\n",
      "  â†³ Valid values: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  â†³ Example: 1\n",
      "\n",
      "â€¢ roi (str, required)\n",
      "  â†³ Region of Interest (ROI) for voxel prediction\n",
      "  â†³ Valid values: ['V1', 'V2', 'V3', 'hV4', 'EBA', 'FBA-2', 'OFA', 'FFA-1', 'FFA-2', 'PPA', 'RSC', 'OPA', 'OWFA', 'VWFA-1', 'VWFA-2', 'mfs-words', 'early', 'midventral', 'midlateral', 'midparietal', 'parietal', 'lateral', 'ventral']\n",
      "  â†³ Example: V1\n",
      "\n",
      "â€¢ nest_dir (str, optional)\n",
      "  â†³ Root directory of the NEST repository (optional if default paths are set)\n",
      "  â†³ Example: ./\n",
      "\n",
      "ðŸ“¦ Example Usage:\n",
      "\n",
      "from nest import NEST\n",
      "\n",
      "nest = NEST(\"/path/to/nest_dir\")\n",
      "model = nest.get_encoding_model(\"fmri_nsd_fwrf\", subject=1, roi='V1', nest_dir='./')\n",
      "\n",
      "# Generate responses (assuming stimulus is a numpy array)\n",
      "responses = model.generate_response(stimulus)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Choose a model of interest\n",
    "model_id = \"fmri_nsd_fwrf\"  # Example model ID\n",
    "\n",
    "# Get comprehensive model information\n",
    "model_info = nest.get_model_info(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 257.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Images shape:\n",
      "(100, 3, 227, 227)\n",
      "(Batch size Ã— 3 RGB Channels x Width x Height)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import transforms as trn\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "images_dir = os.path.join('tutorial_images')\n",
    "images_list = os.listdir(images_dir)\n",
    "images_list.sort()\n",
    "print('Images:')\n",
    "# for i, img in enumerate(images_list):\n",
    "#     print(str(i+1) + ': ' + img)\n",
    "    \n",
    "    \n",
    "images = []\n",
    "for img in tqdm(images_list):\n",
    "    img_dir = os.path.join(images_dir, img)\n",
    "    img = Image.open(img_dir).convert('RGB')\n",
    "    # Center crop the images to square format, and resize them\n",
    "    transform = trn.Compose([\n",
    "        trn.CenterCrop(min(img.size)),\n",
    "        trn.Resize((227,227))\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = np.asarray(img)\n",
    "    img = img.transpose(2,0,1)\n",
    "    images.append(img)\n",
    "images = np.asarray(images)\n",
    "\n",
    "# Print the images dimensions\n",
    "print('\\n\\nImages shape:')\n",
    "print(images.shape)\n",
    "print('(Batch size Ã— 3 RGB Channels x Width x Height)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu for subject 1, ROI V1\n"
     ]
    }
   ],
   "source": [
    "final_model = nest.get_encoding_model(\"fmri_nsd_fwrf\", subject=1, roi=\"V1\", nest_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding fMRI responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s, Encoded images=100, Total images=100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.055599  ,  0.1633554 ,  0.0943293 , ...,  0.01447097,\n",
       "         0.15747474,  0.01369774],\n",
       "       [ 0.01819719,  0.07435445,  0.04977052, ..., -0.01315978,\n",
       "         0.10419706,  0.03225161],\n",
       "       [ 0.10229094,  0.21433882,  0.13944887, ...,  0.01901415,\n",
       "         0.14813517,  0.00760403],\n",
       "       ...,\n",
       "       [ 0.04800354, -0.14399454, -0.28155515, ..., -0.02397447,\n",
       "        -0.08016404,  0.00045429],\n",
       "       [ 0.04686468,  0.07014671,  0.00538955, ..., -0.02637868,\n",
       "         0.10473584,  0.02725478],\n",
       "       [ 0.11645083,  0.02392432, -0.04246235, ..., -0.07162039,\n",
       "        -0.0095483 , -0.01467158]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nest.encode(final_model, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: {'fmri_nsd_fwrf': ['1.0.0']}\n",
      "Available modalities: ['fmri']\n",
      "Model loaded on cpu for subject 1, ROI V1\n"
     ]
    }
   ],
   "source": [
    "from new_nest import NEST\n",
    "nest = NEST(nest_dir=\"./\")\n",
    "\n",
    "# List all available models and their versions\n",
    "available_models = nest.list_models()\n",
    "print(f\"Available models: {available_models}\")\n",
    "\n",
    "# See what modalities are available\n",
    "modalities = nest.which_modalities()\n",
    "print(f\"Available modalities: {modalities}\")\n",
    "\n",
    "final_model = nest.get_encoding_model(\"fmri_nsd_fwrf\", subject=1, roi=\"V1\", nest_dir=\"./\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'FMRIEncodingModel' has no attribute 'get_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_modality\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Repositories/NEST/new_nest/interfaces/base_model.py:167\u001b[0m, in \u001b[0;36mget_modality\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Extract from the metadata\u001b[39;00m\n\u001b[1;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_metadata()\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodality\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'FMRIEncodingModel' has no attribute 'get_metadata'"
     ]
    }
   ],
   "source": [
    "final_model.get_modality()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
